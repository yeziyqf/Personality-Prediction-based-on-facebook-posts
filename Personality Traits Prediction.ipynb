{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import os\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a confusion matrix and\n",
    "# calculate evaluation metrics using it\n",
    "def summarize_metrics(tp, tn, fp, fn):\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    f1_score = (2 * recall * precision) / (recall + precision)\n",
    "\n",
    "    print(\"Precison:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    status_data = pandas.read_csv(\"mypersonality_final.csv\")\n",
    "\n",
    "    NEG_INDEX = 2\n",
    "    POS_INDEX = 3\n",
    "    NEU_INDEX = 4\n",
    "    COMP_INDEX = 5\n",
    "\n",
    "    # Annotate the status with sentiment scores\n",
    "    # From nltk.sentiment.vader corpus\n",
    "    if not os.path.isfile(\"mypersonality_cleaned.csv\"):\n",
    "        status_data.insert(NEG_INDEX, \"sentiNEG\", 0)\n",
    "        status_data.insert(POS_INDEX, \"sentiPOS\", 0)\n",
    "        status_data.insert(NEU_INDEX, \"sentiNEU\", 0)\n",
    "        status_data.insert(COMP_INDEX, \"sentiCOMPOUND\", 0)\n",
    "\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        count = 0\n",
    "        for row in status_data.itertuples():\n",
    "            \"\"\"\n",
    "            pos: positive\n",
    "            neg: negative\n",
    "            neu: neutral\n",
    "            compound: aggregated score for the sentence\n",
    "            \"\"\"\n",
    "            ss = sid.polarity_scores(row.STATUS)\n",
    "            status_data.iloc[count, NEG_INDEX] = ss[\"neg\"]\n",
    "            status_data.iloc[count, POS_INDEX] = ss[\"pos\"]\n",
    "            status_data.iloc[count, NEU_INDEX] = ss[\"neu\"]\n",
    "            status_data.iloc[count, COMP_INDEX] = ss[\"compound\"]\n",
    "            count += 1\n",
    "\n",
    "        status_data.to_csv(\"mypersonality_cleaned.csv\")\n",
    "    else:\n",
    "        status_data = pandas.read_csv(\"mypersonality_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop NAs\n",
    "status_data = status_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We drop columns which give us a score for personality type\n",
    "status_data = status_data.drop(['STATUS', '#AUTHID', 'sEXT', 'sNEU', 'sAGR',\n",
    "                                    'sCON', 'sOPN', 'DATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop non-normalized scores of Brokerage and Betweenness\n",
    "status_data = status_data.drop(['BROKERAGE', 'BETWEENNESS', 'NBROKERAGE',\n",
    "                                    'NBETWEENNESS', 'DENSITY', 'TRANSITIVITY', 'NETWORKSIZE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiNEG</th>\n",
       "      <th>sentiPOS</th>\n",
       "      <th>sentiNEU</th>\n",
       "      <th>sentiCOMPOUND</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-0.3412</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.786</td>\n",
       "      <td>-0.6841</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.5095</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9887</th>\n",
       "      <td>9887</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.4993</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>9888</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9889</th>\n",
       "      <td>9889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>9890</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>9891</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.3648</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9892</th>\n",
       "      <td>9892</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>9893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>9894</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>9895</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.8217</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>9896</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.9036</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>9897</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>9898</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.6083</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>9899</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>9900</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>9901</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9902</th>\n",
       "      <td>9902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9903</th>\n",
       "      <td>9903</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>9904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>9905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>9906</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>9907</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>9908</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>9909</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>9910</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>9911</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>9912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>9913</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>9914</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>9915</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>9916</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9916 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  sentiNEG  sentiPOS  sentiNEU  sentiCOMPOUND cEXT cNEU cAGR  \\\n",
       "0              0     0.000     0.412     0.588         0.4215    n    y    n   \n",
       "1              1     0.167     0.000     0.833        -0.3412    n    y    n   \n",
       "2              2     0.195     0.278     0.527         0.6280    n    y    n   \n",
       "3              3     0.000     0.259     0.741         0.4215    n    y    n   \n",
       "4              4     0.000     0.592     0.408         0.4404    n    y    n   \n",
       "5              5     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "6              6     0.000     0.515     0.485         0.8916    n    y    n   \n",
       "7              7     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "8              8     0.188     0.053     0.759        -0.6249    n    y    n   \n",
       "9              9     0.000     0.323     0.677         0.7351    n    y    n   \n",
       "10            10     0.242     0.000     0.758        -0.1531    n    y    n   \n",
       "11            11     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "12            12     0.136     0.376     0.488         0.6633    n    y    n   \n",
       "13            13     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "14            14     0.000     0.541     0.459         0.7096    n    y    n   \n",
       "15            15     0.000     0.317     0.683         0.4184    n    y    n   \n",
       "16            16     0.000     0.178     0.822         0.3818    n    y    n   \n",
       "17            17     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "18            18     0.266     0.000     0.734        -0.4404    n    y    n   \n",
       "19            19     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "20            20     0.000     0.179     0.821         0.3400    n    y    n   \n",
       "21            21     0.214     0.000     0.786        -0.6841    n    y    n   \n",
       "22            22     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "23            23     0.000     0.430     0.570         0.7003    n    y    n   \n",
       "24            24     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "25            25     0.310     0.000     0.690        -0.5574    n    y    n   \n",
       "26            26     0.000     1.000     0.000         0.4404    n    y    n   \n",
       "27            27     0.351     0.000     0.649        -0.4019    n    y    n   \n",
       "28            28     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "29            29     0.329     0.227     0.445        -0.5095    n    y    n   \n",
       "...          ...       ...       ...       ...            ...  ...  ...  ...   \n",
       "9887        9887     0.144     0.000     0.856        -0.4993    n    y    n   \n",
       "9888        9888     0.000     0.666     0.334         0.7177    n    y    n   \n",
       "9889        9889     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9890        9890     0.000     0.163     0.837         0.5707    n    y    n   \n",
       "9891        9891     0.262     0.158     0.580        -0.3648    n    y    n   \n",
       "9892        9892     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9893        9893     0.000     0.259     0.741         0.4199    n    y    n   \n",
       "9894        9894     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9895        9895     0.136     0.490     0.374         0.8217    n    y    n   \n",
       "9896        9896     0.000     0.603     0.397         0.9036    n    y    n   \n",
       "9897        9897     0.000     0.225     0.775         0.5399    n    y    n   \n",
       "9898        9898     0.407     0.179     0.413        -0.6083    n    y    n   \n",
       "9899        9899     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9900        9900     0.086     0.000     0.914        -0.0572    n    y    n   \n",
       "9901        9901     0.000     0.394     0.606         0.8316    n    y    n   \n",
       "9902        9902     0.000     0.413     0.587         0.9449    y    y    y   \n",
       "9903        9903     0.000     0.286     0.714         0.4215    y    y    y   \n",
       "9904        9904     0.000     0.437     0.563         0.5962    y    y    y   \n",
       "9905        9905     0.000     0.107     0.893         0.4588    y    y    y   \n",
       "9906        9906     0.196     0.324     0.480         0.4939    n    n    n   \n",
       "9907        9907     0.108     0.081     0.811        -0.1779    n    n    n   \n",
       "9908        9908     0.000     0.000     1.000         0.0000    n    n    n   \n",
       "9909        9909     0.266     0.000     0.734        -0.4404    n    y    n   \n",
       "9910        9910     0.000     0.000     1.000         0.0000    y    n    y   \n",
       "9911        9911     0.000     0.000     1.000         0.0000    n    n    y   \n",
       "9912        9912     0.000     0.000     1.000         0.0000    n    n    y   \n",
       "9913        9913     0.000     0.322     0.678         0.2263    y    y    y   \n",
       "9914        9914     0.219     0.000     0.781        -0.1027    y    y    y   \n",
       "9915        9915     0.000     0.000     1.000         0.0000    n    y    n   \n",
       "9916        9916     0.000     0.223     0.777         0.8416    y    y    n   \n",
       "\n",
       "     cCON cOPN  \n",
       "0       n    y  \n",
       "1       n    y  \n",
       "2       n    y  \n",
       "3       n    y  \n",
       "4       n    y  \n",
       "5       n    y  \n",
       "6       n    y  \n",
       "7       n    y  \n",
       "8       n    y  \n",
       "9       n    y  \n",
       "10      n    y  \n",
       "11      n    y  \n",
       "12      n    y  \n",
       "13      n    y  \n",
       "14      n    y  \n",
       "15      n    y  \n",
       "16      n    y  \n",
       "17      n    y  \n",
       "18      n    y  \n",
       "19      n    y  \n",
       "20      n    y  \n",
       "21      n    y  \n",
       "22      n    y  \n",
       "23      n    y  \n",
       "24      n    y  \n",
       "25      n    y  \n",
       "26      n    y  \n",
       "27      n    y  \n",
       "28      n    y  \n",
       "29      n    y  \n",
       "...   ...  ...  \n",
       "9887    y    y  \n",
       "9888    y    y  \n",
       "9889    y    y  \n",
       "9890    y    y  \n",
       "9891    y    y  \n",
       "9892    y    y  \n",
       "9893    y    y  \n",
       "9894    y    y  \n",
       "9895    y    y  \n",
       "9896    y    y  \n",
       "9897    y    y  \n",
       "9898    y    y  \n",
       "9899    y    y  \n",
       "9900    y    y  \n",
       "9901    y    y  \n",
       "9902    y    y  \n",
       "9903    y    y  \n",
       "9904    y    y  \n",
       "9905    y    y  \n",
       "9906    y    y  \n",
       "9907    y    y  \n",
       "9908    y    y  \n",
       "9909    n    y  \n",
       "9910    y    n  \n",
       "9911    n    y  \n",
       "9912    n    y  \n",
       "9913    y    y  \n",
       "9914    y    y  \n",
       "9915    n    y  \n",
       "9916    y    y  \n",
       "\n",
       "[9916 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the name of first row from \"Unknown\" to \"rowID\"\n",
    "new_columns = status_data.columns.values\n",
    "new_columns[0] = \"rowID\"\n",
    "status_data.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the columns to be predicted, at the end\n",
    "cols = status_data.columns.tolist()\n",
    "cols = cols[:5] + cols[5:10]\n",
    "status_data = status_data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowID</th>\n",
       "      <th>sentiNEG</th>\n",
       "      <th>sentiPOS</th>\n",
       "      <th>sentiNEU</th>\n",
       "      <th>sentiCOMPOUND</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-0.3412</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.786</td>\n",
       "      <td>-0.6841</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.5095</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9887</th>\n",
       "      <td>9887</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.4993</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>9888</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9889</th>\n",
       "      <td>9889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>9890</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>9891</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.3648</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9892</th>\n",
       "      <td>9892</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>9893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>9894</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>9895</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.8217</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>9896</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.9036</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>9897</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>9898</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.6083</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>9899</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>9900</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>9901</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9902</th>\n",
       "      <td>9902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9903</th>\n",
       "      <td>9903</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>9904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>9905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>9906</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>9907</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>9908</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>9909</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>9910</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>9911</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>9912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>9913</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>9914</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>9915</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>9916</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9916 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rowID  sentiNEG  sentiPOS  sentiNEU  sentiCOMPOUND cEXT cNEU cAGR cCON  \\\n",
       "0         0     0.000     0.412     0.588         0.4215    n    y    n    n   \n",
       "1         1     0.167     0.000     0.833        -0.3412    n    y    n    n   \n",
       "2         2     0.195     0.278     0.527         0.6280    n    y    n    n   \n",
       "3         3     0.000     0.259     0.741         0.4215    n    y    n    n   \n",
       "4         4     0.000     0.592     0.408         0.4404    n    y    n    n   \n",
       "5         5     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "6         6     0.000     0.515     0.485         0.8916    n    y    n    n   \n",
       "7         7     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "8         8     0.188     0.053     0.759        -0.6249    n    y    n    n   \n",
       "9         9     0.000     0.323     0.677         0.7351    n    y    n    n   \n",
       "10       10     0.242     0.000     0.758        -0.1531    n    y    n    n   \n",
       "11       11     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "12       12     0.136     0.376     0.488         0.6633    n    y    n    n   \n",
       "13       13     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "14       14     0.000     0.541     0.459         0.7096    n    y    n    n   \n",
       "15       15     0.000     0.317     0.683         0.4184    n    y    n    n   \n",
       "16       16     0.000     0.178     0.822         0.3818    n    y    n    n   \n",
       "17       17     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "18       18     0.266     0.000     0.734        -0.4404    n    y    n    n   \n",
       "19       19     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "20       20     0.000     0.179     0.821         0.3400    n    y    n    n   \n",
       "21       21     0.214     0.000     0.786        -0.6841    n    y    n    n   \n",
       "22       22     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "23       23     0.000     0.430     0.570         0.7003    n    y    n    n   \n",
       "24       24     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "25       25     0.310     0.000     0.690        -0.5574    n    y    n    n   \n",
       "26       26     0.000     1.000     0.000         0.4404    n    y    n    n   \n",
       "27       27     0.351     0.000     0.649        -0.4019    n    y    n    n   \n",
       "28       28     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "29       29     0.329     0.227     0.445        -0.5095    n    y    n    n   \n",
       "...     ...       ...       ...       ...            ...  ...  ...  ...  ...   \n",
       "9887   9887     0.144     0.000     0.856        -0.4993    n    y    n    y   \n",
       "9888   9888     0.000     0.666     0.334         0.7177    n    y    n    y   \n",
       "9889   9889     0.000     0.000     1.000         0.0000    n    y    n    y   \n",
       "9890   9890     0.000     0.163     0.837         0.5707    n    y    n    y   \n",
       "9891   9891     0.262     0.158     0.580        -0.3648    n    y    n    y   \n",
       "9892   9892     0.000     0.000     1.000         0.0000    n    y    n    y   \n",
       "9893   9893     0.000     0.259     0.741         0.4199    n    y    n    y   \n",
       "9894   9894     0.000     0.000     1.000         0.0000    n    y    n    y   \n",
       "9895   9895     0.136     0.490     0.374         0.8217    n    y    n    y   \n",
       "9896   9896     0.000     0.603     0.397         0.9036    n    y    n    y   \n",
       "9897   9897     0.000     0.225     0.775         0.5399    n    y    n    y   \n",
       "9898   9898     0.407     0.179     0.413        -0.6083    n    y    n    y   \n",
       "9899   9899     0.000     0.000     1.000         0.0000    n    y    n    y   \n",
       "9900   9900     0.086     0.000     0.914        -0.0572    n    y    n    y   \n",
       "9901   9901     0.000     0.394     0.606         0.8316    n    y    n    y   \n",
       "9902   9902     0.000     0.413     0.587         0.9449    y    y    y    y   \n",
       "9903   9903     0.000     0.286     0.714         0.4215    y    y    y    y   \n",
       "9904   9904     0.000     0.437     0.563         0.5962    y    y    y    y   \n",
       "9905   9905     0.000     0.107     0.893         0.4588    y    y    y    y   \n",
       "9906   9906     0.196     0.324     0.480         0.4939    n    n    n    y   \n",
       "9907   9907     0.108     0.081     0.811        -0.1779    n    n    n    y   \n",
       "9908   9908     0.000     0.000     1.000         0.0000    n    n    n    y   \n",
       "9909   9909     0.266     0.000     0.734        -0.4404    n    y    n    n   \n",
       "9910   9910     0.000     0.000     1.000         0.0000    y    n    y    y   \n",
       "9911   9911     0.000     0.000     1.000         0.0000    n    n    y    n   \n",
       "9912   9912     0.000     0.000     1.000         0.0000    n    n    y    n   \n",
       "9913   9913     0.000     0.322     0.678         0.2263    y    y    y    y   \n",
       "9914   9914     0.219     0.000     0.781        -0.1027    y    y    y    y   \n",
       "9915   9915     0.000     0.000     1.000         0.0000    n    y    n    n   \n",
       "9916   9916     0.000     0.223     0.777         0.8416    y    y    n    y   \n",
       "\n",
       "     cOPN  \n",
       "0       y  \n",
       "1       y  \n",
       "2       y  \n",
       "3       y  \n",
       "4       y  \n",
       "5       y  \n",
       "6       y  \n",
       "7       y  \n",
       "8       y  \n",
       "9       y  \n",
       "10      y  \n",
       "11      y  \n",
       "12      y  \n",
       "13      y  \n",
       "14      y  \n",
       "15      y  \n",
       "16      y  \n",
       "17      y  \n",
       "18      y  \n",
       "19      y  \n",
       "20      y  \n",
       "21      y  \n",
       "22      y  \n",
       "23      y  \n",
       "24      y  \n",
       "25      y  \n",
       "26      y  \n",
       "27      y  \n",
       "28      y  \n",
       "29      y  \n",
       "...   ...  \n",
       "9887    y  \n",
       "9888    y  \n",
       "9889    y  \n",
       "9890    y  \n",
       "9891    y  \n",
       "9892    y  \n",
       "9893    y  \n",
       "9894    y  \n",
       "9895    y  \n",
       "9896    y  \n",
       "9897    y  \n",
       "9898    y  \n",
       "9899    y  \n",
       "9900    y  \n",
       "9901    y  \n",
       "9902    y  \n",
       "9903    y  \n",
       "9904    y  \n",
       "9905    y  \n",
       "9906    y  \n",
       "9907    y  \n",
       "9908    y  \n",
       "9909    y  \n",
       "9910    n  \n",
       "9911    y  \n",
       "9912    y  \n",
       "9913    y  \n",
       "9914    y  \n",
       "9915    y  \n",
       "9916    y  \n",
       "\n",
       "[9916 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'y' for 1 and 'n' for 0\n",
    "features = ['cEXT', 'cNEU', 'cOPN', 'cAGR', 'cCON']\n",
    "for feature in features:\n",
    "    status_data[feature] = status_data[feature].map({'y': 1.0, 'n': 0.0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training and test data: 66% and 33%\n",
    "train_data, test_data = train_test_split(status_data, test_size=0.50)\n",
    "\n",
    "train = train_data.values\n",
    "test = test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a classifier\n",
    "# k is chosen to be square root of number of training example\n",
    "model = KNeighborsClassifier(n_neighbors=250)\n",
    "model = model.fit(train[0:, 1:5], train[0:, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.46200000e+03,   3.89000000e-01,   1.90000000e-01,\n",
       "         4.21000000e-01,  -2.24400000e-01,   0.00000000e+00,\n",
       "         1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         1.00000000e+00])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "output = model.predict(test[:, 1:5])\n",
    "rowID = [TEST.rowID for TEST in test_data.itertuples()]\n",
    "result_df = pandas.DataFrame({\"rowID\": rowID,\"cOPN\": list(output)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3265 121 1148 424\n",
      "Precison: 0.7398595060049853\n",
      "Recall: 0.8850637029005151\n",
      "Accuracy: 0.6829366680112948\n",
      "F1 score: 0.8059738336213281\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for row in rowID:\n",
    "    test_cEXT_val = int(test_data.loc[test_data['rowID'] == row].cOPN)\n",
    "    result_cEXT_val = int(result_df.loc[result_df['rowID'] == row].cOPN)\n",
    "    if test_cEXT_val == 1:\n",
    "        if result_cEXT_val == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_SVM = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_SVM = model_SVM.fit(train[0:, 1:5], train[0:, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict_SVM\n",
    "output_SVM = model_SVM.predict(test[:, 1:5])\n",
    "rowID_SVM = [TEST.rowID for TEST in test_data.itertuples()]\n",
    "result_df_SVM = pandas.DataFrame({\"rowID\": rowID,\"cOPN\": list(output_SVM)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3545 33 1236 144\n",
      "Precison: 0.7414766785191382\n",
      "Recall: 0.9609650311737599\n",
      "Accuracy: 0.7216619604679306\n",
      "F1 score: 0.8370720188902008\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for row in rowID:\n",
    "    test_cEXT_val = int(test_data.loc[test_data['rowID'] == row].cOPN)\n",
    "    result_cEXT_val_SVM = int(result_df_SVM.loc[result_df['rowID'] == row].cOPN)\n",
    "    if test_cEXT_val == 1:\n",
    "        if result_cEXT_val_SVM == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_SVM == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model_gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_gnb = model_gnb.fit(train[0:, 1:5], train[0:, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.46200000e+03,   3.89000000e-01,   1.90000000e-01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  4.54600000e+03,   2.71000000e-01,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  5.78300000e+03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  9.20000000e+03,   3.58000000e-01,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   1.00000000e+00],\n",
       "       [  9.84000000e+03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   1.00000000e+00],\n",
       "       [  8.50900000e+03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict_gnb\n",
    "output_gnb = model_gnb.predict(test[:, 1:5])\n",
    "rowID_gnb = [TEST.rowID for TEST in test_data.itertuples()]\n",
    "result_df_gnb = pandas.DataFrame({\"rowID\": rowID,\"cOPN\": list(output_gnb)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2935 240 1029 754\n",
      "Precison: 0.7404137235116044\n",
      "Recall: 0.795608566007048\n",
      "Accuracy: 0.6403791851553046\n",
      "F1 score: 0.7670194694890893\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for row in rowID:\n",
    "    test_cEXT_val = int(test_data.loc[test_data['rowID'] == row].cOPN)\n",
    "    result_cEXT_val_gnb = int(result_df_gnb.loc[result_df['rowID'] == row].cOPN)\n",
    "    if test_cEXT_val == 1:\n",
    "        if result_cEXT_val_gnb == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_gnb == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Second Approach, using Doc2Vec and LSTM for predicting\n",
    "import gensim\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "docLabels_y = []\n",
    "docLabels_y = [f for f in listdir(\"./Dataset Processed/txt output_final/y\") if f.endswith('.txt')]\n",
    "docLabels_n = []\n",
    "docLabels_n = [f for f in listdir(\"./Dataset Processed/txt output_final/n\") if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "data_y = []\n",
    "for doc in docLabels_y:\n",
    "    f = open(\"./Dataset Processed/txt output_final/y/\" + doc, 'r')\n",
    "    data_y.append(f)\n",
    "    data.append(f)\n",
    "    f.close()\n",
    "data_n = []\n",
    "for doc in docLabels_n:\n",
    "    f1 = open(\"./Dataset Processed/txt output_final/n/\" + doc, 'r')\n",
    "    data_n.append(f1)\n",
    "    data.append(f1)\n",
    "    f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LabeledSentence = gensim.models.doc2vec.LabeledSentence  \n",
    "  \n",
    "# class LabeledLineSentence(object):  \n",
    "#     def __init__(self, sentences):  \n",
    "#         self.sentences = sentences  \n",
    "#     def __iter__(self):  \n",
    "#         for id, line in enumerate(sentences):  \n",
    "#             yield LabeledSentence(words=line, tags=['SENT_%s' % id])  \n",
    "#     def sentences_perm(self):\n",
    "#         shuffle(self.sentences)\n",
    "#         return self.sentences\n",
    "              \n",
    "# it = LabeledLineSentence(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 00:00:04,754 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c25d30b7e267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0municode\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \"\"\"\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initial survey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build tables & arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, documents, progress_per, trim_rule, update)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-6043c56a5bd9>\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mLabeledSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SENT_%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msentences_perm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Doc2Vec(size=100, window=10, min_count=1, workers=11, alpha=0.025, min_alpha=0.025)  \n",
    "model.build_vocab(it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-24 23:58:19,747 : INFO : running C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py -f C:\\Users\\tianx\\AppData\\Roaming\\jupyter\\runtime\\kernel-1525645a-5d42-4290-ad5f-0978609f7a3f.json\n",
      "2018-04-24 23:58:19,752 : INFO : Epoch 0\n",
      "2018-04-24 23:58:19,770 : INFO : training model with 11 workers on 0 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-cf447f146636>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     model.train(it.sentences_perm(),\n\u001b[0;32m     14\u001b[0m                 \u001b[1;31m#total_examples=model.corpus_count,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first finalize vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from random import shuffle\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "for epoch in range(100):\n",
    "    logger.info('Epoch %d' % epoch)\n",
    "    model.train(it.sentences_perm(),\n",
    "                #total_examples=model.corpus_count,\n",
    "                epochs=model.iter,\n",
    "    )\n",
    "\n",
    "model.save('./Doc2Vec_model.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "        flipped = {}\n",
    "\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(\n",
    "                        utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 14:42:12,740 : INFO : running C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py -f C:\\Users\\tianx\\AppData\\Roaming\\jupyter\\runtime\\kernel-2864b082-d451-44be-9948-6e712fdda201.json\n",
      "2018-04-25 14:42:12,893 : INFO : collecting all words and their counts\n",
      "2018-04-25 14:42:12,897 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-04-25 14:42:12,973 : INFO : collected 15770 word types and 5364 unique tags from a corpus of 5364 examples and 72630 words\n",
      "2018-04-25 14:42:12,981 : INFO : Loading a fresh vocabulary\n",
      "2018-04-25 14:42:13,096 : INFO : min_count=1 retains 15770 unique words (100% of original 15770, drops 0)\n",
      "2018-04-25 14:42:13,103 : INFO : min_count=1 leaves 72630 word corpus (100% of original 72630, drops 0)\n",
      "2018-04-25 14:42:13,282 : INFO : deleting the raw counts dictionary of 15770 items\n",
      "2018-04-25 14:42:13,287 : INFO : sample=0.0001 downsamples 406 most-common words\n",
      "2018-04-25 14:42:13,291 : INFO : downsampling leaves estimated 41246 word corpus (56.8% of prior 72630)\n",
      "2018-04-25 14:42:13,295 : INFO : estimated required memory for 15770 words and 100 dimensions: 23719400 bytes\n",
      "2018-04-25 14:42:13,503 : INFO : resetting layer weights\n",
      "2018-04-25 14:42:14,127 : INFO : Epoch 0\n",
      "2018-04-25 14:42:14,143 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:15,358 : INFO : PROGRESS: at 21.91% examples, 44599 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:16,369 : INFO : PROGRESS: at 60.23% examples, 65143 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:16,901 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:16,911 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:16,919 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:16,931 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:16,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:16,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:17,012 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:17,018 : INFO : training on 363150 raw words (233054 effective words) took 2.8s, 82933 effective words/s\n",
      "2018-04-25 14:42:17,022 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:17,025 : INFO : Epoch 1\n",
      "2018-04-25 14:42:17,041 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:18,055 : INFO : PROGRESS: at 21.94% examples, 51267 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:19,077 : INFO : PROGRESS: at 63.12% examples, 72841 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:19,596 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:19,604 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:19,609 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:19,622 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:19,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:19,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:19,685 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:19,691 : INFO : training on 363150 raw words (233292 effective words) took 2.6s, 88525 effective words/s\n",
      "2018-04-25 14:42:19,696 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:19,699 : INFO : Epoch 2\n",
      "2018-04-25 14:42:19,710 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:20,725 : INFO : PROGRESS: at 22.02% examples, 51536 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:21,761 : INFO : PROGRESS: at 63.34% examples, 72501 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:22,238 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:22,250 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:22,257 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:22,266 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:22,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:22,298 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:22,335 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:22,342 : INFO : training on 363150 raw words (233342 effective words) took 2.6s, 89101 effective words/s\n",
      "2018-04-25 14:42:22,345 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:22,348 : INFO : Epoch 3\n",
      "2018-04-25 14:42:22,360 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:23,687 : INFO : PROGRESS: at 41.28% examples, 73559 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:24,667 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:24,724 : INFO : PROGRESS: at 87.99% examples, 87481 words/s, in_qsize 5, out_qsize 1\n",
      "2018-04-25 14:42:24,734 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:24,744 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:24,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:24,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:24,773 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:24,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:24,808 : INFO : training on 363150 raw words (232834 effective words) took 2.4s, 95920 effective words/s\n",
      "2018-04-25 14:42:24,811 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:24,813 : INFO : Epoch 4\n",
      "2018-04-25 14:42:24,826 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:25,861 : INFO : PROGRESS: at 38.56% examples, 88087 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:27,115 : INFO : PROGRESS: at 79.78% examples, 81685 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:42:27,163 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:27,170 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:27,175 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:27,181 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:27,207 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:27,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:27,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:27,283 : INFO : training on 363150 raw words (232845 effective words) took 2.4s, 95286 effective words/s\n",
      "2018-04-25 14:42:27,287 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:27,290 : INFO : Epoch 5\n",
      "2018-04-25 14:42:27,307 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:28,336 : INFO : PROGRESS: at 35.79% examples, 82673 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:29,568 : INFO : PROGRESS: at 79.81% examples, 83042 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:42:29,633 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:29,649 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:29,656 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:29,663 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:29,695 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:29,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:29,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:29,740 : INFO : training on 363150 raw words (232957 effective words) took 2.4s, 96655 effective words/s\n",
      "2018-04-25 14:42:29,743 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:29,746 : INFO : Epoch 6\n",
      "2018-04-25 14:42:29,759 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:30,826 : INFO : PROGRESS: at 21.96% examples, 48439 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:31,875 : INFO : PROGRESS: at 60.48% examples, 66931 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:32,398 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:32,402 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:32,411 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:32,421 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:32,427 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:32,450 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:32,478 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:32,481 : INFO : training on 363150 raw words (232939 effective words) took 2.7s, 85921 effective words/s\n",
      "2018-04-25 14:42:32,483 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:32,486 : INFO : Epoch 7\n",
      "2018-04-25 14:42:32,500 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:33,942 : INFO : PROGRESS: at 41.11% examples, 67601 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:34,809 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:34,817 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:34,859 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:34,869 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:34,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:34,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:34,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:34,944 : INFO : training on 363150 raw words (233188 effective words) took 2.4s, 96060 effective words/s\n",
      "2018-04-25 14:42:34,947 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:34,951 : INFO : Epoch 8\n",
      "2018-04-25 14:42:34,961 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:36,316 : INFO : PROGRESS: at 41.25% examples, 72011 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:37,167 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:37,171 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:37,180 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:37,188 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:37,195 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:37,229 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:37,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:37,265 : INFO : training on 363150 raw words (233182 effective words) took 2.3s, 102058 effective words/s\n",
      "2018-04-25 14:42:37,268 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:37,270 : INFO : Epoch 9\n",
      "2018-04-25 14:42:37,281 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:38,587 : INFO : PROGRESS: at 41.20% examples, 74358 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:39,484 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:39,509 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:39,517 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:39,529 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:39,537 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:39,550 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:39,586 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:39,592 : INFO : training on 363150 raw words (232904 effective words) took 2.3s, 101469 effective words/s\n",
      "2018-04-25 14:42:39,596 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:39,601 : INFO : Epoch 10\n",
      "2018-04-25 14:42:39,614 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:40,930 : INFO : PROGRESS: at 41.17% examples, 74239 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:41,825 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:41,838 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:41,856 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:41,887 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:41,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:41,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:41,956 : INFO : PROGRESS: at 100.00% examples, 100393 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:42:41,964 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:41,967 : INFO : training on 363150 raw words (233178 effective words) took 2.3s, 99915 effective words/s\n",
      "2018-04-25 14:42:41,972 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:41,975 : INFO : Epoch 11\n",
      "2018-04-25 14:42:41,991 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:43,293 : INFO : PROGRESS: at 41.16% examples, 74791 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:44,234 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:44,240 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:44,248 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:44,258 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:44,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:44,289 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:44,323 : INFO : PROGRESS: at 100.00% examples, 100666 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:42:44,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:44,331 : INFO : training on 363150 raw words (232961 effective words) took 2.3s, 100329 effective words/s\n",
      "2018-04-25 14:42:44,334 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:44,337 : INFO : Epoch 12\n",
      "2018-04-25 14:42:44,348 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:45,705 : INFO : PROGRESS: at 41.25% examples, 71582 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:46,634 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:46,643 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:46,648 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:46,655 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:46,663 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:46,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:46,718 : INFO : PROGRESS: at 100.00% examples, 98949 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:42:46,722 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:46,725 : INFO : training on 363150 raw words (233069 effective words) took 2.4s, 98676 effective words/s\n",
      "2018-04-25 14:42:46,727 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:46,730 : INFO : Epoch 13\n",
      "2018-04-25 14:42:46,741 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:48,062 : INFO : PROGRESS: at 41.23% examples, 74088 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:48,946 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:48,950 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:48,960 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:48,969 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:48,979 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:49,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:49,068 : INFO : PROGRESS: at 100.00% examples, 101127 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:42:49,074 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:49,079 : INFO : training on 363150 raw words (233434 effective words) took 2.3s, 100668 effective words/s\n",
      "2018-04-25 14:42:49,083 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:49,086 : INFO : Epoch 14\n",
      "2018-04-25 14:42:49,103 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:50,491 : INFO : PROGRESS: at 41.14% examples, 69901 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:51,359 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:51,365 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:51,373 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:51,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:51,387 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:51,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:51,462 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:51,466 : INFO : training on 363150 raw words (232873 effective words) took 2.3s, 99111 effective words/s\n",
      "2018-04-25 14:42:51,469 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:51,472 : INFO : Epoch 15\n",
      "2018-04-25 14:42:51,485 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:52,561 : INFO : PROGRESS: at 21.88% examples, 48428 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:53,693 : INFO : PROGRESS: at 60.56% examples, 64386 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:42:54,252 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:54,278 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:54,284 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:54,314 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:54,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:54,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:54,377 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:54,384 : INFO : training on 363150 raw words (232793 effective words) took 2.9s, 80851 effective words/s\n",
      "2018-04-25 14:42:54,387 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:54,390 : INFO : Epoch 16\n",
      "2018-04-25 14:42:54,404 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:55,456 : INFO : PROGRESS: at 24.70% examples, 55526 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:56,568 : INFO : PROGRESS: at 60.51% examples, 65606 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:42:57,139 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:42:57,188 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:42:57,206 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:42:57,246 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:42:57,261 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:42:57,278 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:42:57,333 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:42:57,344 : INFO : training on 363150 raw words (232912 effective words) took 2.9s, 79607 effective words/s\n",
      "2018-04-25 14:42:57,348 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:42:57,354 : INFO : Epoch 17\n",
      "2018-04-25 14:42:57,381 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:42:58,685 : INFO : PROGRESS: at 22.04% examples, 41053 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:00,224 : INFO : PROGRESS: at 60.50% examples, 50557 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:00,896 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:00,962 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:00,987 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:01,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:01,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:01,056 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:01,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:01,135 : INFO : training on 363150 raw words (233067 effective words) took 3.7s, 62956 effective words/s\n",
      "2018-04-25 14:43:01,139 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:01,142 : INFO : Epoch 18\n",
      "2018-04-25 14:43:01,161 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:02,435 : INFO : PROGRESS: at 21.89% examples, 41711 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:03,677 : INFO : PROGRESS: at 60.47% examples, 57052 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:04,357 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:04,387 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:04,437 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:04,444 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:04,448 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:04,451 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:04,476 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:04,479 : INFO : training on 363150 raw words (232988 effective words) took 3.3s, 71232 effective words/s\n",
      "2018-04-25 14:43:04,484 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:04,487 : INFO : Epoch 19\n",
      "2018-04-25 14:43:04,498 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:05,622 : INFO : PROGRESS: at 21.99% examples, 46127 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:06,936 : INFO : PROGRESS: at 79.77% examples, 76806 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:43:06,978 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:06,985 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:06,999 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:07,018 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:07,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:07,054 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:07,086 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:07,090 : INFO : training on 363150 raw words (233387 effective words) took 2.6s, 90498 effective words/s\n",
      "2018-04-25 14:43:07,093 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:07,095 : INFO : Epoch 20\n",
      "2018-04-25 14:43:07,108 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:08,166 : INFO : PROGRESS: at 30.29% examples, 68258 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:09,314 : INFO : PROGRESS: at 60.41% examples, 64641 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:09,829 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:09,841 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:09,848 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:09,856 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:09,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:09,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:09,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:09,944 : INFO : training on 363150 raw words (232984 effective words) took 2.8s, 82914 effective words/s\n",
      "2018-04-25 14:43:09,947 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:09,950 : INFO : Epoch 21\n",
      "2018-04-25 14:43:09,966 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:11,466 : INFO : PROGRESS: at 22.04% examples, 34557 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:12,595 : INFO : PROGRESS: at 60.37% examples, 53941 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:13,241 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:13,249 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:13,269 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:13,292 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:13,303 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:13,334 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:13,387 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:13,394 : INFO : training on 363150 raw words (233291 effective words) took 3.4s, 68358 effective words/s\n",
      "2018-04-25 14:43:13,397 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:13,405 : INFO : Epoch 22\n",
      "2018-04-25 14:43:13,428 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:14,626 : INFO : PROGRESS: at 22.02% examples, 43789 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:15,659 : INFO : PROGRESS: at 57.69% examples, 61060 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:16,569 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:16,592 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:16,611 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:16,621 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:16,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:16,666 : INFO : PROGRESS: at 97.10% examples, 70551 words/s, in_qsize 1, out_qsize 1\n",
      "2018-04-25 14:43:16,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:16,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:16,732 : INFO : training on 363150 raw words (233137 effective words) took 3.3s, 71121 effective words/s\n",
      "2018-04-25 14:43:16,735 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:16,745 : INFO : Epoch 23\n",
      "2018-04-25 14:43:16,768 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:18,216 : INFO : PROGRESS: at 21.83% examples, 46117 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:19,217 : INFO : PROGRESS: at 71.45% examples, 79000 words/s, in_qsize 11, out_qsize 0\n",
      "2018-04-25 14:43:19,667 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:19,673 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:19,720 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:19,729 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:19,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:19,742 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:19,766 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:19,772 : INFO : training on 363150 raw words (232937 effective words) took 2.7s, 87417 effective words/s\n",
      "2018-04-25 14:43:19,775 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:19,779 : INFO : Epoch 24\n",
      "2018-04-25 14:43:19,790 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:20,953 : INFO : PROGRESS: at 21.98% examples, 44921 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:22,001 : INFO : PROGRESS: at 60.47% examples, 64384 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:22,519 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:22,537 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:22,547 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:22,578 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:22,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:22,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:22,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:22,644 : INFO : training on 363150 raw words (233128 effective words) took 2.8s, 82327 effective words/s\n",
      "2018-04-25 14:43:22,646 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:22,649 : INFO : Epoch 25\n",
      "2018-04-25 14:43:22,661 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:23,689 : INFO : PROGRESS: at 24.62% examples, 56886 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:24,694 : INFO : PROGRESS: at 68.54% examples, 79238 words/s, in_qsize 12, out_qsize 0\n",
      "2018-04-25 14:43:25,161 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:25,166 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:25,223 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:25,231 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:25,248 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:25,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:25,302 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:25,309 : INFO : training on 363150 raw words (232780 effective words) took 2.6s, 88316 effective words/s\n",
      "2018-04-25 14:43:25,312 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:25,316 : INFO : Epoch 26\n",
      "2018-04-25 14:43:25,332 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:26,361 : INFO : PROGRESS: at 27.33% examples, 63168 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:27,363 : INFO : PROGRESS: at 71.34% examples, 82606 words/s, in_qsize 11, out_qsize 0\n",
      "2018-04-25 14:43:27,854 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:27,869 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:27,880 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:27,886 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:27,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:27,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:27,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:28,006 : INFO : training on 363150 raw words (233019 effective words) took 2.7s, 87775 effective words/s\n",
      "2018-04-25 14:43:28,009 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:28,013 : INFO : Epoch 27\n",
      "2018-04-25 14:43:28,031 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:29,128 : INFO : PROGRESS: at 21.93% examples, 48367 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:30,132 : INFO : PROGRESS: at 68.71% examples, 77726 words/s, in_qsize 12, out_qsize 0\n",
      "2018-04-25 14:43:30,643 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:30,661 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:30,667 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:30,674 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:30,679 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:30,714 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:30,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:30,768 : INFO : training on 363150 raw words (233155 effective words) took 2.7s, 86335 effective words/s\n",
      "2018-04-25 14:43:30,772 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:30,775 : INFO : Epoch 28\n",
      "2018-04-25 14:43:30,792 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:31,857 : INFO : PROGRESS: at 21.90% examples, 49076 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:32,858 : INFO : PROGRESS: at 76.91% examples, 87791 words/s, in_qsize 9, out_qsize 0\n",
      "2018-04-25 14:43:33,317 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:33,333 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:33,343 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:33,352 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:33,359 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:33,393 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:33,429 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:33,435 : INFO : training on 363150 raw words (233265 effective words) took 2.6s, 88920 effective words/s\n",
      "2018-04-25 14:43:33,438 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:33,441 : INFO : Epoch 29\n",
      "2018-04-25 14:43:33,456 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:34,528 : INFO : PROGRESS: at 21.76% examples, 48655 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:35,535 : INFO : PROGRESS: at 68.59% examples, 77759 words/s, in_qsize 12, out_qsize 0\n",
      "2018-04-25 14:43:36,093 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:36,102 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:36,107 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:36,127 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:36,140 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:36,175 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:36,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:36,214 : INFO : training on 363150 raw words (233115 effective words) took 2.7s, 85140 effective words/s\n",
      "2018-04-25 14:43:36,216 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:36,219 : INFO : Epoch 30\n",
      "2018-04-25 14:43:36,240 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:37,258 : INFO : PROGRESS: at 38.56% examples, 89692 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:38,470 : INFO : PROGRESS: at 79.77% examples, 83988 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:43:38,491 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:38,545 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:38,561 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:38,584 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:38,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:38,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:38,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:38,648 : INFO : training on 363150 raw words (233046 effective words) took 2.4s, 97430 effective words/s\n",
      "2018-04-25 14:43:38,651 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:38,655 : INFO : Epoch 31\n",
      "2018-04-25 14:43:38,667 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:39,799 : INFO : PROGRESS: at 24.65% examples, 51737 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:40,844 : INFO : PROGRESS: at 74.07% examples, 80050 words/s, in_qsize 10, out_qsize 0\n",
      "2018-04-25 14:43:41,191 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:41,206 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:41,235 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:41,275 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:41,286 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:41,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:41,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:41,331 : INFO : training on 363150 raw words (233075 effective words) took 2.6s, 87962 effective words/s\n",
      "2018-04-25 14:43:41,334 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:41,337 : INFO : Epoch 32\n",
      "2018-04-25 14:43:41,352 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:42,737 : INFO : PROGRESS: at 41.23% examples, 70237 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:43,617 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:43,636 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:43,648 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:43,657 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:43,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:43,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:43,718 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:43,724 : INFO : training on 363150 raw words (233134 effective words) took 2.4s, 98895 effective words/s\n",
      "2018-04-25 14:43:43,728 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:43,733 : INFO : Epoch 33\n",
      "2018-04-25 14:43:43,747 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:45,103 : INFO : PROGRESS: at 41.15% examples, 71817 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:45,968 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:45,984 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:45,990 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:46,004 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:46,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:46,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:46,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:46,084 : INFO : training on 363150 raw words (233019 effective words) took 2.3s, 100585 effective words/s\n",
      "2018-04-25 14:43:46,088 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:46,092 : INFO : Epoch 34\n",
      "2018-04-25 14:43:46,103 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:47,128 : INFO : PROGRESS: at 32.73% examples, 76361 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:48,356 : INFO : PROGRESS: at 79.61% examples, 83148 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:43:48,377 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:48,386 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:48,393 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:48,395 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:48,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:48,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:48,483 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:48,486 : INFO : training on 363150 raw words (232694 effective words) took 2.4s, 98466 effective words/s\n",
      "2018-04-25 14:43:48,489 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:48,492 : INFO : Epoch 35\n",
      "2018-04-25 14:43:48,506 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:49,877 : INFO : PROGRESS: at 41.24% examples, 70921 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:50,762 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:50,777 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:50,799 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:50,804 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:50,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:50,835 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:50,870 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:50,874 : INFO : training on 363150 raw words (232997 effective words) took 2.3s, 99164 effective words/s\n",
      "2018-04-25 14:43:50,877 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:50,881 : INFO : Epoch 36\n",
      "2018-04-25 14:43:50,894 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:52,269 : INFO : PROGRESS: at 41.07% examples, 70355 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:43:53,127 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:53,154 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:53,189 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:53,199 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:53,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:53,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:53,271 : INFO : PROGRESS: at 100.00% examples, 98557 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:43:53,280 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:53,284 : INFO : training on 363150 raw words (233034 effective words) took 2.4s, 98012 effective words/s\n",
      "2018-04-25 14:43:53,289 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:53,292 : INFO : Epoch 37\n",
      "2018-04-25 14:43:53,304 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:54,655 : INFO : PROGRESS: at 41.23% examples, 71829 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:55,498 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:55,503 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:55,512 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:55,520 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:55,523 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:55,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:55,589 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:55,594 : INFO : training on 363150 raw words (233000 effective words) took 2.3s, 102330 effective words/s\n",
      "2018-04-25 14:43:55,598 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:55,600 : INFO : Epoch 38\n",
      "2018-04-25 14:43:55,611 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:56,978 : INFO : PROGRESS: at 41.23% examples, 70880 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:43:57,831 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:43:57,835 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:43:57,848 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:43:57,855 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:43:57,866 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:43:57,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:43:57,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:43:57,961 : INFO : training on 363150 raw words (232842 effective words) took 2.3s, 99591 effective words/s\n",
      "2018-04-25 14:43:57,964 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:43:57,969 : INFO : Epoch 39\n",
      "2018-04-25 14:43:57,985 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:43:59,317 : INFO : PROGRESS: at 41.24% examples, 73229 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:00,196 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:00,226 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:00,237 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:00,252 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:00,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:00,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:00,334 : INFO : PROGRESS: at 100.00% examples, 100022 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:44:00,337 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:00,339 : INFO : training on 363150 raw words (233084 effective words) took 2.3s, 99780 effective words/s\n",
      "2018-04-25 14:44:00,342 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:00,345 : INFO : Epoch 40\n",
      "2018-04-25 14:44:00,356 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:01,699 : INFO : PROGRESS: at 41.22% examples, 72499 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:02,591 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:02,600 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:02,616 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:02,624 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:02,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:02,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:02,708 : INFO : PROGRESS: at 100.00% examples, 99894 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:44:02,711 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:02,714 : INFO : training on 363150 raw words (233126 effective words) took 2.3s, 99648 effective words/s\n",
      "2018-04-25 14:44:02,719 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:02,722 : INFO : Epoch 41\n",
      "2018-04-25 14:44:02,738 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:04,052 : INFO : PROGRESS: at 41.19% examples, 74044 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:04,957 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:04,962 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:04,968 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:04,973 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:04,978 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:05,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:05,037 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:05,042 : INFO : training on 363150 raw words (232945 effective words) took 2.3s, 101695 effective words/s\n",
      "2018-04-25 14:44:05,045 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:05,048 : INFO : Epoch 42\n",
      "2018-04-25 14:44:05,061 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:06,399 : INFO : PROGRESS: at 41.21% examples, 72454 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:07,247 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:07,252 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:07,272 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:07,282 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:07,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:07,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:07,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:07,355 : INFO : training on 363150 raw words (232956 effective words) took 2.3s, 102156 effective words/s\n",
      "2018-04-25 14:44:07,357 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:07,361 : INFO : Epoch 43\n",
      "2018-04-25 14:44:07,373 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:08,717 : INFO : PROGRESS: at 41.29% examples, 72178 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:09,578 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:09,584 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:09,616 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:09,623 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:09,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:09,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:09,684 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:09,689 : INFO : training on 363150 raw words (232823 effective words) took 2.3s, 101040 effective words/s\n",
      "2018-04-25 14:44:09,692 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:09,695 : INFO : Epoch 44\n",
      "2018-04-25 14:44:09,706 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:11,070 : INFO : PROGRESS: at 41.16% examples, 71559 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:11,901 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:11,905 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:11,921 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:11,927 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:11,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:11,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:12,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:12,039 : INFO : training on 363150 raw words (233285 effective words) took 2.3s, 100859 effective words/s\n",
      "2018-04-25 14:44:12,043 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:12,047 : INFO : Epoch 45\n",
      "2018-04-25 14:44:12,060 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:13,420 : INFO : PROGRESS: at 41.34% examples, 71670 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:14,289 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:14,299 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:14,307 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:14,316 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:14,324 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:14,351 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:14,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:14,386 : INFO : training on 363150 raw words (232988 effective words) took 2.3s, 100851 effective words/s\n",
      "2018-04-25 14:44:14,389 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:14,391 : INFO : Epoch 46\n",
      "2018-04-25 14:44:14,405 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:15,755 : INFO : PROGRESS: at 41.21% examples, 71858 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:16,604 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:16,610 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:16,621 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:16,650 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:16,656 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:16,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:16,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:16,718 : INFO : training on 363150 raw words (232966 effective words) took 2.3s, 101265 effective words/s\n",
      "2018-04-25 14:44:16,721 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:16,724 : INFO : Epoch 47\n",
      "2018-04-25 14:44:16,736 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:18,045 : INFO : PROGRESS: at 41.01% examples, 74172 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:18,935 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:18,947 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:18,961 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:18,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:19,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:19,021 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:19,047 : INFO : PROGRESS: at 100.00% examples, 101543 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:44:19,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:19,055 : INFO : training on 363150 raw words (233124 effective words) took 2.3s, 101177 effective words/s\n",
      "2018-04-25 14:44:19,057 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:19,059 : INFO : Epoch 48\n",
      "2018-04-25 14:44:19,071 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:20,421 : INFO : PROGRESS: at 41.11% examples, 72399 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:21,279 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:21,287 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:21,300 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:21,309 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:21,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:21,342 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:21,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:21,377 : INFO : training on 363150 raw words (233129 effective words) took 2.3s, 102046 effective words/s\n",
      "2018-04-25 14:44:21,379 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:21,382 : INFO : Epoch 49\n",
      "2018-04-25 14:44:21,397 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:22,770 : INFO : PROGRESS: at 41.28% examples, 70988 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:23,623 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:23,636 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:23,653 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:23,676 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:23,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:23,700 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:23,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:23,733 : INFO : training on 363150 raw words (233056 effective words) took 2.3s, 100575 effective words/s\n",
      "2018-04-25 14:44:23,736 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:23,739 : INFO : Epoch 50\n",
      "2018-04-25 14:44:23,754 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:25,078 : INFO : PROGRESS: at 41.23% examples, 73229 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:25,981 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:25,995 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:25,999 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:26,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:26,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:26,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:26,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:26,072 : INFO : training on 363150 raw words (232849 effective words) took 2.3s, 100999 effective words/s\n",
      "2018-04-25 14:44:26,076 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:26,079 : INFO : Epoch 51\n",
      "2018-04-25 14:44:26,093 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:27,409 : INFO : PROGRESS: at 41.19% examples, 74886 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:28,284 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:28,312 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:28,323 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:28,332 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:28,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:28,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:28,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:28,397 : INFO : training on 363150 raw words (233162 effective words) took 2.3s, 102580 effective words/s\n",
      "2018-04-25 14:44:28,400 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:28,404 : INFO : Epoch 52\n",
      "2018-04-25 14:44:28,416 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:29,727 : INFO : PROGRESS: at 41.26% examples, 74129 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:30,601 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:30,606 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:30,614 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:30,645 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:30,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:30,666 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:30,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:30,709 : INFO : training on 363150 raw words (233136 effective words) took 2.3s, 102308 effective words/s\n",
      "2018-04-25 14:44:30,712 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:30,715 : INFO : Epoch 53\n",
      "2018-04-25 14:44:30,727 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:31,741 : INFO : PROGRESS: at 38.55% examples, 89453 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:32,908 : INFO : PROGRESS: at 79.75% examples, 85750 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:32,953 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:32,966 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:32,983 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:32,992 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:33,007 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:33,026 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:33,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:33,067 : INFO : training on 363150 raw words (232904 effective words) took 2.3s, 100105 effective words/s\n",
      "2018-04-25 14:44:33,074 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:33,077 : INFO : Epoch 54\n",
      "2018-04-25 14:44:33,089 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:34,452 : INFO : PROGRESS: at 41.17% examples, 71570 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:35,281 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:35,308 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:35,334 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:35,339 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:35,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:35,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:35,407 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:35,411 : INFO : training on 363150 raw words (232986 effective words) took 2.3s, 101256 effective words/s\n",
      "2018-04-25 14:44:35,414 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:35,417 : INFO : Epoch 55\n",
      "2018-04-25 14:44:35,429 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:36,463 : INFO : PROGRESS: at 35.60% examples, 82070 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:37,727 : INFO : PROGRESS: at 79.69% examples, 81533 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:37,745 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:37,752 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:37,755 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:37,802 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:37,809 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:37,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:37,869 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:37,876 : INFO : training on 363150 raw words (232969 effective words) took 2.4s, 95928 effective words/s\n",
      "2018-04-25 14:44:37,879 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:37,882 : INFO : Epoch 56\n",
      "2018-04-25 14:44:37,899 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:38,960 : INFO : PROGRESS: at 22.11% examples, 49287 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:40,234 : INFO : PROGRESS: at 79.78% examples, 80298 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:40,312 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:40,321 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:40,326 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:40,338 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:40,345 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:40,365 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:40,394 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:40,399 : INFO : training on 363150 raw words (233047 effective words) took 2.5s, 93989 effective words/s\n",
      "2018-04-25 14:44:40,402 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:40,405 : INFO : Epoch 57\n",
      "2018-04-25 14:44:40,420 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:41,741 : INFO : PROGRESS: at 41.17% examples, 73799 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:42,600 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:42,611 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:42,617 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:42,661 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:42,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:42,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:42,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:42,719 : INFO : training on 363150 raw words (233402 effective words) took 2.3s, 102266 effective words/s\n",
      "2018-04-25 14:44:42,722 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:42,726 : INFO : Epoch 58\n",
      "2018-04-25 14:44:42,738 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:43,752 : INFO : PROGRESS: at 38.45% examples, 89631 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:44,969 : INFO : PROGRESS: at 79.77% examples, 83843 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:44,995 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:45,000 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:45,007 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:45,031 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:45,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:45,065 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:45,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:45,106 : INFO : training on 363150 raw words (233182 effective words) took 2.4s, 99005 effective words/s\n",
      "2018-04-25 14:44:45,110 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:45,114 : INFO : Epoch 59\n",
      "2018-04-25 14:44:45,128 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:46,146 : INFO : PROGRESS: at 32.95% examples, 76316 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:47,248 : INFO : PROGRESS: at 79.77% examples, 88168 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:47,376 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:47,423 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:47,445 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:47,464 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:47,471 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:47,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:47,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:47,502 : INFO : training on 363150 raw words (232966 effective words) took 2.4s, 98655 effective words/s\n",
      "2018-04-25 14:44:47,504 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:47,508 : INFO : Epoch 60\n",
      "2018-04-25 14:44:47,521 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:48,858 : INFO : PROGRESS: at 41.22% examples, 73113 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:44:49,733 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:49,746 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:49,752 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:49,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:49,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:49,795 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:49,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:49,834 : INFO : training on 363150 raw words (233307 effective words) took 2.3s, 101731 effective words/s\n",
      "2018-04-25 14:44:49,837 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:49,840 : INFO : Epoch 61\n",
      "2018-04-25 14:44:49,852 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:50,873 : INFO : PROGRESS: at 38.40% examples, 89601 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:52,006 : INFO : PROGRESS: at 79.42% examples, 86979 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:44:52,036 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:52,043 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:52,050 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:52,060 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:52,103 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:52,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:52,153 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:52,156 : INFO : training on 363150 raw words (233087 effective words) took 2.3s, 101972 effective words/s\n",
      "2018-04-25 14:44:52,159 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:52,161 : INFO : Epoch 62\n",
      "2018-04-25 14:44:52,171 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:53,557 : INFO : PROGRESS: at 41.07% examples, 70210 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:54,415 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:54,421 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:54,426 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:54,435 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:54,444 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:54,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:54,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:54,510 : INFO : training on 363150 raw words (233332 effective words) took 2.3s, 100472 effective words/s\n",
      "2018-04-25 14:44:54,514 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:54,518 : INFO : Epoch 63\n",
      "2018-04-25 14:44:54,534 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:55,876 : INFO : PROGRESS: at 41.18% examples, 72625 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:56,746 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:56,763 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:56,776 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:56,784 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:56,791 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:56,813 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:56,842 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:56,848 : INFO : training on 363150 raw words (233008 effective words) took 2.3s, 101537 effective words/s\n",
      "2018-04-25 14:44:56,852 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:56,855 : INFO : Epoch 64\n",
      "2018-04-25 14:44:56,869 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:44:58,220 : INFO : PROGRESS: at 41.24% examples, 71971 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:44:59,058 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:44:59,063 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:44:59,069 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:44:59,072 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:44:59,100 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:44:59,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:44:59,181 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:44:59,188 : INFO : training on 363150 raw words (232816 effective words) took 2.3s, 101136 effective words/s\n",
      "2018-04-25 14:44:59,192 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:44:59,196 : INFO : Epoch 65\n",
      "2018-04-25 14:44:59,209 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:00,236 : INFO : PROGRESS: at 30.13% examples, 70016 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:01,441 : INFO : PROGRESS: at 79.79% examples, 84138 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:01,516 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:01,530 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:01,540 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:01,549 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:01,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:01,572 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:01,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:01,616 : INFO : training on 363150 raw words (233168 effective words) took 2.4s, 97767 effective words/s\n",
      "2018-04-25 14:45:01,619 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:01,622 : INFO : Epoch 66\n",
      "2018-04-25 14:45:01,635 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:02,648 : INFO : PROGRESS: at 32.94% examples, 76987 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:03,781 : INFO : PROGRESS: at 79.80% examples, 87260 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:03,853 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:03,904 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:03,911 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:03,931 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:03,942 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:03,952 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:03,985 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:03,990 : INFO : training on 363150 raw words (233414 effective words) took 2.3s, 99711 effective words/s\n",
      "2018-04-25 14:45:03,993 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:03,996 : INFO : Epoch 67\n",
      "2018-04-25 14:45:04,007 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:05,033 : INFO : PROGRESS: at 38.46% examples, 88293 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:06,262 : INFO : PROGRESS: at 79.60% examples, 82780 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:06,275 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:06,284 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:06,295 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:06,305 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:06,312 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:06,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:06,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:06,386 : INFO : training on 363150 raw words (232974 effective words) took 2.4s, 98397 effective words/s\n",
      "2018-04-25 14:45:06,388 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:06,391 : INFO : Epoch 68\n",
      "2018-04-25 14:45:06,403 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:07,705 : INFO : PROGRESS: at 41.20% examples, 74458 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:08,654 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:08,659 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:08,666 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:08,673 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:08,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:08,695 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:08,724 : INFO : PROGRESS: at 100.00% examples, 100916 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:45:08,729 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:08,731 : INFO : training on 363150 raw words (232961 effective words) took 2.3s, 100589 effective words/s\n",
      "2018-04-25 14:45:08,734 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:08,740 : INFO : Epoch 69\n",
      "2018-04-25 14:45:08,754 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:09,770 : INFO : PROGRESS: at 32.79% examples, 76714 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:10,922 : INFO : PROGRESS: at 79.74% examples, 86269 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:10,960 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:11,034 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:11,040 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:11,050 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:11,061 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:11,070 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:11,102 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:11,107 : INFO : training on 363150 raw words (233169 effective words) took 2.3s, 99555 effective words/s\n",
      "2018-04-25 14:45:11,112 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:11,116 : INFO : Epoch 70\n",
      "2018-04-25 14:45:11,129 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:12,500 : INFO : PROGRESS: at 41.19% examples, 71051 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:13,369 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:13,378 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:13,398 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:13,404 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:13,414 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:13,439 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:13,468 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:13,473 : INFO : training on 363150 raw words (233266 effective words) took 2.3s, 100261 effective words/s\n",
      "2018-04-25 14:45:13,476 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:13,478 : INFO : Epoch 71\n",
      "2018-04-25 14:45:13,493 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:14,890 : INFO : PROGRESS: at 41.11% examples, 69742 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:15,728 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:15,742 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:15,747 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:15,754 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:15,762 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:15,788 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:15,833 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:15,836 : INFO : training on 363150 raw words (233156 effective words) took 2.3s, 100296 effective words/s\n",
      "2018-04-25 14:45:15,841 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:15,845 : INFO : Epoch 72\n",
      "2018-04-25 14:45:15,860 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:17,221 : INFO : PROGRESS: at 41.11% examples, 71179 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:18,080 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:18,088 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:18,099 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:18,111 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:18,120 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:18,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:18,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:18,193 : INFO : training on 363150 raw words (233128 effective words) took 2.3s, 100432 effective words/s\n",
      "2018-04-25 14:45:18,195 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:18,201 : INFO : Epoch 73\n",
      "2018-04-25 14:45:18,213 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:19,562 : INFO : PROGRESS: at 41.20% examples, 71913 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:20,436 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:20,443 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:20,446 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:20,462 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:20,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:20,496 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:20,527 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:20,533 : INFO : training on 363150 raw words (232788 effective words) took 2.3s, 100848 effective words/s\n",
      "2018-04-25 14:45:20,536 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:20,538 : INFO : Epoch 74\n",
      "2018-04-25 14:45:20,552 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:21,932 : INFO : PROGRESS: at 41.28% examples, 70402 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:22,810 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:22,820 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:22,831 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:22,840 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:22,847 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:22,878 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:22,916 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:22,923 : INFO : training on 363150 raw words (233133 effective words) took 2.4s, 98864 effective words/s\n",
      "2018-04-25 14:45:22,927 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:22,931 : INFO : Epoch 75\n",
      "2018-04-25 14:45:22,946 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:24,358 : INFO : PROGRESS: at 41.19% examples, 68792 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:25,207 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:25,213 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:25,259 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:25,271 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:25,283 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:25,297 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:25,328 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:25,333 : INFO : training on 363150 raw words (233277 effective words) took 2.4s, 98318 effective words/s\n",
      "2018-04-25 14:45:25,336 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:25,339 : INFO : Epoch 76\n",
      "2018-04-25 14:45:25,354 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:26,753 : INFO : PROGRESS: at 41.30% examples, 70278 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:27,616 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:27,630 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:27,640 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:27,646 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:27,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:27,686 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:27,724 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:27,729 : INFO : training on 363150 raw words (233435 effective words) took 2.3s, 99413 effective words/s\n",
      "2018-04-25 14:45:27,732 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:27,735 : INFO : Epoch 77\n",
      "2018-04-25 14:45:27,746 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:28,779 : INFO : PROGRESS: at 35.78% examples, 82384 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:29,990 : INFO : PROGRESS: at 79.77% examples, 83482 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:30,073 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:30,089 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:30,105 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:30,112 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:30,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:30,142 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:30,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:30,179 : INFO : training on 363150 raw words (232843 effective words) took 2.4s, 96432 effective words/s\n",
      "2018-04-25 14:45:30,182 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:30,185 : INFO : Epoch 78\n",
      "2018-04-25 14:45:30,195 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:31,210 : INFO : PROGRESS: at 29.92% examples, 70349 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:32,478 : INFO : PROGRESS: at 79.81% examples, 81961 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:32,506 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:32,514 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:32,568 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:32,574 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:32,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:32,600 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:32,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:32,634 : INFO : training on 363150 raw words (233299 effective words) took 2.4s, 96138 effective words/s\n",
      "2018-04-25 14:45:32,638 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:32,642 : INFO : Epoch 79\n",
      "2018-04-25 14:45:32,655 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:33,677 : INFO : PROGRESS: at 29.88% examples, 69586 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:34,872 : INFO : PROGRESS: at 79.78% examples, 84328 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:34,888 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:34,915 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:34,960 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:34,976 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:34,985 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:34,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:35,041 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:35,044 : INFO : training on 363150 raw words (233180 effective words) took 2.4s, 98082 effective words/s\n",
      "2018-04-25 14:45:35,048 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:35,050 : INFO : Epoch 80\n",
      "2018-04-25 14:45:35,061 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:36,175 : INFO : PROGRESS: at 21.93% examples, 46720 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:37,506 : INFO : PROGRESS: at 79.79% examples, 76389 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:37,548 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:37,554 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:37,557 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:37,568 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:37,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:37,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:37,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:37,642 : INFO : training on 363150 raw words (232809 effective words) took 2.6s, 90683 effective words/s\n",
      "2018-04-25 14:45:37,644 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:37,647 : INFO : Epoch 81\n",
      "2018-04-25 14:45:37,660 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:39,059 : INFO : PROGRESS: at 41.33% examples, 69616 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:39,895 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:39,909 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:39,917 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:39,925 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:39,935 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:39,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:39,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:39,997 : INFO : training on 363150 raw words (232979 effective words) took 2.3s, 100468 effective words/s\n",
      "2018-04-25 14:45:39,999 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:40,002 : INFO : Epoch 82\n",
      "2018-04-25 14:45:40,014 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:41,032 : INFO : PROGRESS: at 30.10% examples, 70017 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:42,221 : INFO : PROGRESS: at 79.79% examples, 84708 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:42,266 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:42,305 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:42,310 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:42,330 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:42,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:42,352 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:42,387 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:42,392 : INFO : training on 363150 raw words (233009 effective words) took 2.4s, 98532 effective words/s\n",
      "2018-04-25 14:45:42,395 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:42,399 : INFO : Epoch 83\n",
      "2018-04-25 14:45:42,413 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:43,435 : INFO : PROGRESS: at 24.67% examples, 57830 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:44,644 : INFO : PROGRESS: at 79.75% examples, 84147 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:44,681 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:44,710 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:44,746 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:44,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:44,780 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:44,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:44,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:44,825 : INFO : training on 363150 raw words (233199 effective words) took 2.4s, 97498 effective words/s\n",
      "2018-04-25 14:45:44,828 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:44,830 : INFO : Epoch 84\n",
      "2018-04-25 14:45:44,848 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:46,260 : INFO : PROGRESS: at 41.17% examples, 68899 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:47,108 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:47,115 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:47,120 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:47,126 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:47,134 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:47,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:47,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:47,214 : INFO : training on 363150 raw words (233202 effective words) took 2.3s, 99367 effective words/s\n",
      "2018-04-25 14:45:47,218 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:47,221 : INFO : Epoch 85\n",
      "2018-04-25 14:45:47,234 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:48,252 : INFO : PROGRESS: at 35.64% examples, 82963 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:49,451 : INFO : PROGRESS: at 79.59% examples, 84424 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:49,476 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:49,481 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:49,485 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:49,507 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:49,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:49,543 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:49,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:49,584 : INFO : training on 363150 raw words (233316 effective words) took 2.3s, 99807 effective words/s\n",
      "2018-04-25 14:45:49,587 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:49,589 : INFO : Epoch 86\n",
      "2018-04-25 14:45:49,601 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:50,672 : INFO : PROGRESS: at 22.01% examples, 48482 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:51,738 : INFO : PROGRESS: at 60.51% examples, 66316 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:52,322 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:52,347 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:52,381 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:52,390 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:52,397 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:52,416 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:52,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:52,452 : INFO : training on 363150 raw words (232907 effective words) took 2.8s, 82068 effective words/s\n",
      "2018-04-25 14:45:52,455 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:52,458 : INFO : Epoch 87\n",
      "2018-04-25 14:45:52,468 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:53,482 : INFO : PROGRESS: at 30.14% examples, 70423 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:45:54,706 : INFO : PROGRESS: at 79.50% examples, 83536 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:54,781 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:54,787 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:54,791 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:54,796 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:54,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:54,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:54,864 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:54,870 : INFO : training on 363150 raw words (233140 effective words) took 2.4s, 97555 effective words/s\n",
      "2018-04-25 14:45:54,874 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:54,877 : INFO : Epoch 88\n",
      "2018-04-25 14:45:54,891 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:55,917 : INFO : PROGRESS: at 27.42% examples, 63214 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:57,171 : INFO : PROGRESS: at 79.78% examples, 82098 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:57,212 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:57,226 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:57,230 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:57,236 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:57,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:57,269 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:57,306 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:57,311 : INFO : training on 363150 raw words (233276 effective words) took 2.4s, 96879 effective words/s\n",
      "2018-04-25 14:45:57,315 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:57,319 : INFO : Epoch 89\n",
      "2018-04-25 14:45:57,335 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:45:58,373 : INFO : PROGRESS: at 38.55% examples, 87719 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:45:59,581 : INFO : PROGRESS: at 79.77% examples, 83260 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:45:59,640 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:45:59,646 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:45:59,654 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:45:59,682 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:45:59,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:45:59,706 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:45:59,736 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:45:59,741 : INFO : training on 363150 raw words (232894 effective words) took 2.4s, 97455 effective words/s\n",
      "2018-04-25 14:45:59,744 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:45:59,746 : INFO : Epoch 90\n",
      "2018-04-25 14:45:59,762 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:00,787 : INFO : PROGRESS: at 30.11% examples, 69968 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:02,086 : INFO : PROGRESS: at 79.66% examples, 80570 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:02,113 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:02,129 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:02,135 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:02,140 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:02,169 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:02,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:02,222 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:02,228 : INFO : training on 363150 raw words (232959 effective words) took 2.4s, 95171 effective words/s\n",
      "2018-04-25 14:46:02,230 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:02,235 : INFO : Epoch 91\n",
      "2018-04-25 14:46:02,246 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:03,434 : INFO : PROGRESS: at 21.88% examples, 43627 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:04,493 : INFO : PROGRESS: at 60.25% examples, 63081 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:46:04,971 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:04,976 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:04,984 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:04,995 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:05,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:05,030 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:05,073 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:05,079 : INFO : training on 363150 raw words (233060 effective words) took 2.8s, 82637 effective words/s\n",
      "2018-04-25 14:46:05,082 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:05,086 : INFO : Epoch 92\n",
      "2018-04-25 14:46:05,097 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:06,483 : INFO : PROGRESS: at 41.04% examples, 70201 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:07,336 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:07,350 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:07,359 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:07,366 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:07,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:07,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:07,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:07,441 : INFO : training on 363150 raw words (232940 effective words) took 2.3s, 100180 effective words/s\n",
      "2018-04-25 14:46:07,444 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:07,446 : INFO : Epoch 93\n",
      "2018-04-25 14:46:07,457 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:08,481 : INFO : PROGRESS: at 35.67% examples, 82784 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:09,633 : INFO : PROGRESS: at 79.74% examples, 86056 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:09,695 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:09,738 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:09,754 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:09,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:09,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:09,789 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:09,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:09,819 : INFO : training on 363150 raw words (232883 effective words) took 2.3s, 99305 effective words/s\n",
      "2018-04-25 14:46:09,822 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:09,827 : INFO : Epoch 94\n",
      "2018-04-25 14:46:09,838 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:10,863 : INFO : PROGRESS: at 38.41% examples, 88834 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:12,095 : INFO : PROGRESS: at 79.71% examples, 82859 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:12,119 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:12,128 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:12,136 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:12,142 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:12,147 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:12,179 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:12,219 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:12,227 : INFO : training on 363150 raw words (232917 effective words) took 2.4s, 98141 effective words/s\n",
      "2018-04-25 14:46:12,229 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:12,233 : INFO : Epoch 95\n",
      "2018-04-25 14:46:12,244 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:13,263 : INFO : PROGRESS: at 35.48% examples, 82761 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:14,472 : INFO : PROGRESS: at 79.33% examples, 83933 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:14,504 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:14,510 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:14,516 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:14,547 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:14,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:14,588 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:14,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:14,628 : INFO : training on 363150 raw words (233145 effective words) took 2.4s, 98351 effective words/s\n",
      "2018-04-25 14:46:14,631 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:14,634 : INFO : Epoch 96\n",
      "2018-04-25 14:46:14,650 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:15,666 : INFO : PROGRESS: at 38.56% examples, 89649 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:16,818 : INFO : PROGRESS: at 79.72% examples, 86349 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:16,907 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:16,927 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:16,943 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:16,951 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:16,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:16,985 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:17,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:17,025 : INFO : training on 363150 raw words (233109 effective words) took 2.4s, 98766 effective words/s\n",
      "2018-04-25 14:46:17,028 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:17,032 : INFO : Epoch 97\n",
      "2018-04-25 14:46:17,047 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:18,069 : INFO : PROGRESS: at 27.49% examples, 63808 words/s, in_qsize 14, out_qsize 0\n",
      "2018-04-25 14:46:19,341 : INFO : PROGRESS: at 79.77% examples, 81617 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-25 14:46:19,353 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:19,365 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:19,380 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:19,392 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:19,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:19,421 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:19,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:19,465 : INFO : training on 363150 raw words (233272 effective words) took 2.4s, 97095 effective words/s\n",
      "2018-04-25 14:46:19,468 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:19,470 : INFO : Epoch 98\n",
      "2018-04-25 14:46:19,480 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:20,921 : INFO : PROGRESS: at 41.23% examples, 67646 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:21,755 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:21,761 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:21,764 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:21,772 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:21,778 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:21,814 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:21,854 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:21,860 : INFO : training on 363150 raw words (233257 effective words) took 2.4s, 98762 effective words/s\n",
      "2018-04-25 14:46:21,863 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:21,866 : INFO : Epoch 99\n",
      "2018-04-25 14:46:21,876 : INFO : training model with 7 workers on 15770 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2018-04-25 14:46:23,259 : INFO : PROGRESS: at 40.97% examples, 70455 words/s, in_qsize 13, out_qsize 0\n",
      "2018-04-25 14:46:24,168 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-04-25 14:46:24,175 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-04-25 14:46:24,183 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-04-25 14:46:24,192 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-25 14:46:24,209 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-25 14:46:24,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-25 14:46:24,291 : INFO : PROGRESS: at 100.00% examples, 97503 words/s, in_qsize 0, out_qsize 1\n",
      "2018-04-25 14:46:24,304 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-25 14:46:24,306 : INFO : training on 363150 raw words (233235 effective words) took 2.4s, 96885 effective words/s\n",
      "2018-04-25 14:46:24,310 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-04-25 14:46:24,315 : INFO : saving Doc2Vec object under ./imdb.d2v, separately None\n",
      "2018-04-25 14:46:24,321 : INFO : not storing attribute syn0norm\n",
      "2018-04-25 14:46:24,326 : INFO : not storing attribute cum_table\n",
      "2018-04-25 14:46:24,763 : INFO : saved ./imdb.d2v\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# shuffle\n",
    "from random import shuffle\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import _pickle as pickle\n",
    "#import cPickle as pickle   #Note: in python3, _pickle was used instead of cpickle\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "        flipped = {}\n",
    "\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(\n",
    "                        utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "\n",
    "sources = {'./Dataset Processed/txt output_final/n/sum/n_all_test.txt':'TEST_NEG', './Dataset Processed/txt output_final/y/sum/y_all_test.txt':'TEST_POS', './Dataset Processed/txt output_final/n/sum/n_all_train.txt':'TRAIN_NEG', './Dataset Processed/txt output_final/y/sum/y_all_train.txt':'TRAIN_POS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)\n",
    "\n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "\n",
    "for epoch in range(100):\n",
    "    logger.info('Epoch %d' % epoch)\n",
    "    model.train(sentences.sentences_perm(),\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter,\n",
    "    )\n",
    "\n",
    "model.save('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 14:48:12,542 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('orleans.', 0.6345005035400391),\n",
       " ('glow', 0.6184808015823364),\n",
       " ('sticks', 0.5736362934112549),\n",
       " ('funk...i', 0.5610934495925903),\n",
       " ('cod4mw2', 0.5446567535400391),\n",
       " ('frog.', 0.5442479848861694),\n",
       " ('soo', 0.5269519090652466),\n",
       " ('convos', 0.51676344871521),\n",
       " ('buddies...', 0.4994044899940491),\n",
       " ('simplest', 0.4900153875350952)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('wow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x29138053710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loser!!!!', 0.6319977045059204),\n",
       " ('booga', 0.6215949058532715),\n",
       " ('courage!', 0.6092036962509155),\n",
       " ('booga!', 0.6054763793945312),\n",
       " ('me!!!', 0.6025193929672241),\n",
       " ('burg...someone', 0.5988327264785767),\n",
       " ('stranded', 0.5970384478569031),\n",
       " ('Mulan...', 0.5725835561752319),\n",
       " ('???', 0.5678666234016418),\n",
       " ('boom', 0.5648619532585144)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numbers!', 0.783098578453064),\n",
       " ('needz', 0.7755323648452759),\n",
       " ('phone,', 0.7491334676742554),\n",
       " ('number?', 0.7489640712738037),\n",
       " ('fo', 0.7310982942581177),\n",
       " ('what?', 0.7217720746994019),\n",
       " ('birfday,', 0.7177667021751404),\n",
       " ('gotz', 0.6787748336791992),\n",
       " ('diamonds', 0.675055742263794),\n",
       " ('divide//', 0.6602415442466736)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('yo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('haha..', 0.61693274974823),\n",
       " ('compell', 0.6105270385742188),\n",
       " ('scrabble', 0.5972521901130676),\n",
       " ('pack...we', 0.594215989112854),\n",
       " ('Yay...', 0.5926961898803711),\n",
       " ('Jesus:', 0.5818872451782227),\n",
       " ('Day.', 0.578163206577301),\n",
       " ('Columbus', 0.5687612295150757),\n",
       " ('gresy', 0.5616876482963562),\n",
       " ('boyfriend', 0.5551649332046509)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('ass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Independence.', 0.6609710454940796),\n",
       " ('Humboldt', 0.6597853899002075),\n",
       " ('declaration', 0.6468594670295715),\n",
       " ('wow...', 0.6215463876724243),\n",
       " ('stories...', 0.6150673031806946),\n",
       " ('Massachusetts.', 0.6051443219184875),\n",
       " ('involving', 0.6025294661521912),\n",
       " ('TJ', 0.5930542945861816),\n",
       " ('aching,', 0.5917971134185791),\n",
       " ('ease', 0.5917605757713318)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TRAGEDY!!!!!!!!!!!', 0.7178655862808228),\n",
       " ('car....yo!', 0.7166972160339355),\n",
       " ('claimed', 0.7024919986724854),\n",
       " ('frnt', 0.6944730281829834),\n",
       " ('evry1!!!!', 0.6852693557739258),\n",
       " ('rockin', 0.6767828464508057),\n",
       " ('statistics.', 0.6531293392181396),\n",
       " ('style', 0.6427193284034729),\n",
       " ('perform', 0.6240640878677368),\n",
       " ('door..mwawaahh', 0.6224106550216675)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((3578,100))\n",
    "train_labels = numpy.zeros((3578))\n",
    "for i in range(1627):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_pos]\n",
    "    train_labels[i] = 1\n",
    "for i in range(1949):\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i+1627] = model.docvecs[prefix_train_neg]\n",
    "    train_labels[i+1627] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.3739661e-01, -2.1580108e-01, -4.8335126e-01,  1.1482675e+00,\n",
       "       -4.7941580e-02, -4.4821692e-01,  3.9039350e-01, -1.2100271e+00,\n",
       "       -2.7058744e-01,  4.6899474e-01, -7.1310796e-02, -7.3290646e-01,\n",
       "        3.4350872e-01, -9.1766286e-01, -8.0835754e-01, -6.1130118e-02,\n",
       "        6.5094030e-01,  1.5644893e-01,  9.8188169e-02,  2.9968953e-01,\n",
       "       -9.5501977e-01, -4.3682599e-01, -5.8097118e-01, -6.4757466e-01,\n",
       "       -4.0122551e-01,  3.5669097e-01,  3.6208892e-01,  2.3384538e-01,\n",
       "       -3.6113268e-01,  1.4055296e-03,  2.6255829e-02,  2.3620425e-02,\n",
       "       -3.5107261e-01, -1.4189466e+00,  3.8090906e-01, -5.7261753e-01,\n",
       "        7.8484339e-01, -6.0194635e-01,  7.6612008e-01, -4.1130096e-01,\n",
       "       -5.9421396e-01, -4.5939643e-02,  6.4430970e-01,  6.5054297e-01,\n",
       "       -3.9242661e-01, -1.8527058e-01,  9.4514400e-01,  4.1067389e-01,\n",
       "       -2.3936620e-01,  1.1376140e+00,  6.2980860e-01,  6.9525754e-01,\n",
       "       -4.5010954e-01,  1.2126705e-01, -6.7568219e-01, -3.6877517e-02,\n",
       "        7.5955963e-01, -3.3801693e-01, -7.0519507e-01, -4.9974373e-01,\n",
       "        1.4324425e-01,  3.2810995e-01,  9.3200046e-01,  1.5225296e-01,\n",
       "       -1.3543957e-01, -7.6429829e-02, -2.3031063e-01, -3.7903610e-01,\n",
       "       -5.5908805e-01, -8.4772456e-01, -4.9993908e-01,  8.3023953e-01,\n",
       "       -6.8701231e-01, -1.1085374e-02, -6.1952090e-01, -1.0657929e+00,\n",
       "        7.9374272e-01,  4.9138966e-01,  3.9554825e-01, -1.7643749e+00,\n",
       "       -3.7373623e-01,  8.6625886e-01, -4.9034629e-02, -9.5129527e-02,\n",
       "        3.3110461e-01,  1.0784267e+00,  1.1681069e+00, -1.0257393e+00,\n",
       "       -7.1068859e-01,  7.4806117e-02, -8.1650037e-01,  4.4953513e-01,\n",
       "        2.7567148e-01,  3.2543078e-01, -1.7744846e+00, -1.4016666e-01,\n",
       "        6.2570167e-01,  1.0729808e+00,  3.3692315e-01,  4.2961814e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TRAIN_NEG_1949']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[1628])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80370075  0.38103873 -0.41746134  0.28001085  0.12257605 -0.27522427\n",
      " -0.0759928   0.44818515  0.1857688  -0.58987349  0.98279178 -0.37254193\n",
      "  0.69479209 -0.15024538 -0.49857754  0.13502854  0.16639599 -0.14383537\n",
      "  0.21795294 -0.09338123 -0.62064594 -0.35703933  0.41458052 -0.79077935\n",
      " -0.57588673 -0.33772352 -1.13289046  0.28825644 -1.06246459 -0.04594202\n",
      "  0.85050446  0.65740472  0.04554173 -0.65110093  0.01392455 -0.00168079\n",
      "  0.01303594 -0.55539489  0.28322324 -0.03220263 -0.17849761 -0.55766553\n",
      "  0.67388767 -1.28418255 -0.62294739  0.09353312 -0.70558155 -0.82815075\n",
      " -0.74368787  0.45761847  0.19435069  0.36589673 -1.32936645 -0.04594028\n",
      " -0.15314017 -0.48406509 -0.2448221   0.0884188   0.11982979 -0.07147586\n",
      " -0.48553157  1.28627694  0.18354133  0.16572593  0.71938622  0.07535198\n",
      " -0.03131915 -1.17586052 -0.45244339 -0.53905445 -0.6917423  -0.23952721\n",
      "  0.70727831 -0.68113184 -0.76739806 -0.61074823  0.9355523  -0.76929212\n",
      "  0.29574177 -0.11356615 -0.2456612  -0.02541027 -0.9174915  -0.19077982\n",
      "  0.50409907  0.15028279  0.81076699 -0.38397548 -0.09198571  0.7155205\n",
      " -0.51208287 -0.68841803  0.8181867  -0.24773917 -0.87482268  0.13176565\n",
      " -0.12423912  0.80236799  0.29084954  0.40151706]\n"
     ]
    }
   ],
   "source": [
    "print(train_arrays[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((1790,100))\n",
    "test_labels = numpy.zeros((1790))\n",
    "for i in range(814):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_pos]\n",
    "    test_labels[i] = 1\n",
    "for i in range(974):\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i+814] = model.docvecs[prefix_test_neg]\n",
    "    test_labels[i+814] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[815])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM setup\n",
    "max_features = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "maxlen = 120\n",
    "batch_size = 64\n",
    "nb_classes = 2   # remaining question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (3578, 100)\n",
      "test shape: (1790, 100)\n"
     ]
    }
   ],
   "source": [
    "print('train shape:', train_arrays.shape)\n",
    "print('test shape:', test_arrays.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label shape: (3578,)\n",
      "test label shape: (1790,)\n"
     ]
    }
   ],
   "source": [
    "print('train label shape:', train_labels.shape)\n",
    "print('test label shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(train_labels, nb_classes)\n",
    "Y_test = np_utils.to_categorical(test_labels, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4495672 , -0.25423512, -0.56111175, ...,  0.29188001,\n",
       "        -0.06568915,  0.37901598],\n",
       "       [-0.56589675,  0.6087445 , -0.00931993, ...,  0.32722384,\n",
       "         0.59142751,  0.05790269],\n",
       "       [-0.55978441,  0.29878759, -0.20974684, ...,  0.07978857,\n",
       "         0.49747965,  0.24056254],\n",
       "       ...,\n",
       "       [-0.29563537,  0.79836869, -0.34482062, ...,  0.28694791,\n",
       "         0.2336712 ,  0.08923047],\n",
       "       [-0.61417621,  0.45285723, -0.38387924, ...,  0.17011005,\n",
       "         0.41078898, -0.38659084],\n",
       "       [ 0.20200892,  0.3076812 ,  0.72610861, ..., -0.64545828,\n",
       "        -0.08853067, -0.0299416 ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:15: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, recurrent_dropout=0.2, dropout=0.2)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-9c30d9fad4f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#                     dropout=0.2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;31m# create an input layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'batch_input_shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                     raise ValueError('The first layer in a '\n\u001b[0m\u001b[0;32m    433\u001b[0m                                      \u001b[1;34m'Sequential model must '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                                      \u001b[1;34m'get an `input_shape` or '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "\n",
    "print('Build LSTM model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(100, 128,dropout=0.2))\n",
    "# model.add(Embedding(\n",
    "# #                     output_dim=EMBEDDING_DIM,\n",
    "# #                     input_dim=100,\n",
    "#                     input_length = maxlen,\n",
    "#                     dropout=0.2))\n",
    "\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(train_arrays, Y_train, batch_size=batch_size, nb_epoch=100,\n",
    "          validation_data=(test_arrays, Y_test))\n",
    "score, acc = model.evaluate(test_arrays,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(test_arrays, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5631284916201117"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a classifier\n",
    "# k is chosen to be square root of number of training example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "model = KNeighborsClassifier(n_neighbors=250)\n",
    "model = model.fit(train_arrays, train_labels)\n",
    "model.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5363128491620112"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm_vector = SVC()\n",
    "model_svm_vector = model_svm_vector.fit(train_arrays, train_labels)\n",
    "model_svm_vector.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_labels[0]\n",
    "# result_df_svm_vector[0][2]\n",
    "# result_df_svm_vector.loc[0]\n",
    "# output_svm_vector[1]\n",
    "# len(test_labels)\n",
    "len(output_svm_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 665 310 520\n",
      "Precison: 0.4867549668874172\n",
      "Recall: 0.36117936117936117\n",
      "Accuracy: 0.5360536612632756\n",
      "F1 score: 0.41466854724964736\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "import pandas\n",
    "output_svm_vector = model_svm_vector.predict(test_arrays)\n",
    "result_df_svm_vector = pandas.DataFrame(output_svm_vector)\n",
    "# print(test_labels)\n",
    "# print(result_df_svm_vector)\n",
    "\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for i in range(0,1789):\n",
    "    test_cEXT_val_vector = int(test_labels[i])\n",
    "    result_cEXT_val_vector = int(output_svm_vector[i])\n",
    "    if test_cEXT_val_vector == 1:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5631284916201117"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a KNN classifier\n",
    "# k is chosen to be square root of number of training example\n",
    "model_knn_vector = KNeighborsClassifier(n_neighbors=250)\n",
    "model_knn_vector = model_knn_vector.fit(train_arrays, train_labels)\n",
    "model_knn_vector.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 900 75 707\n",
      "Precison: 0.5879120879120879\n",
      "Recall: 0.13144963144963145\n",
      "Accuracy: 0.562884292901062\n",
      "F1 score: 0.21485943775100402\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "import pandas\n",
    "output_knn_vector = model_knn_vector.predict(test_arrays)\n",
    "result_df_knn_vector = pandas.DataFrame(output_knn_vector)\n",
    "# print(test_labels)\n",
    "# print(result_df_svm_vector)\n",
    "\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for i in range(0,1789):\n",
    "    test_cEXT_val_vector = int(test_labels[i])\n",
    "    result_cEXT_val_vector = int(output_knn_vector[i])\n",
    "    if test_cEXT_val_vector == 1:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4212290502793296"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model_gnb_vector = GaussianNB()\n",
    "model_gnb_vector = model_gnb_vector.fit(train_arrays, train_labels)\n",
    "model_gnb_vector.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 276 699 336\n",
      "Precison: 0.40611724723874254\n",
      "Recall: 0.5872235872235873\n",
      "Accuracy: 0.42146450531022916\n",
      "F1 score: 0.48016072325464587\n"
     ]
    }
   ],
   "source": [
    "# Build the confusion matrix to assess the model\n",
    "import pandas\n",
    "output_gnb_vector = model_gnb_vector.predict(test_arrays)\n",
    "result_df_gnb_vector = pandas.DataFrame(output_gnb_vector)\n",
    "# print(test_labels)\n",
    "# print(result_df_svm_vector)\n",
    "\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for i in range(0,1789):\n",
    "    test_cEXT_val_vector = int(test_labels[i])\n",
    "    result_cEXT_val_vector = int(output_gnb_vector[i])\n",
    "    if test_cEXT_val_vector == 1:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fn_count += 1\n",
    "    else:\n",
    "        if result_cEXT_val_vector == 1:\n",
    "            fp_count += 1\n",
    "        else:\n",
    "            tn_count += 1\n",
    "\n",
    "print(tp_count, tn_count, fp_count, fn_count)\n",
    "summarize_metrics(tp_count, tn_count, fp_count, fn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try CNN\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Convolution1D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-5e2f418cb45d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# we add a Convolution1D, which will learn nb_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# word group filters of size filter_length:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m model.add(Convolution1D(nb_filter=nb_filter,\n\u001b[0m\u001b[0;32m      7\u001b[0m                         \u001b[0mfilter_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mborder_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Convolution1D' is not defined"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "model.add(Dense(hidden_dims)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using another approach for LSTM\n",
    "max_features = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "nb_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    status_data = pandas.read_csv(\"./Dataset Processed/mypersonality_final_sorted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop NAs\n",
    "status_data = status_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sentiNEG</th>\n",
       "      <th>sentiPOS</th>\n",
       "      <th>sentiNEU</th>\n",
       "      <th>sentiCOMPOUND</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>...</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-0.3412</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/15/09 01:15 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/22/09 04:48 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/20/09 02:31 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>www.thejokerblogs.com</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/16/09 03:21 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>saw a nun zombie, and liked it. Also, *PROPNAM...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/27/09 05:41 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is in Kentucky. 421 miles into her 1100 mile j...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/18/09 06:34 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>was about to finish a digital painting before ...</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/09/09 02:58 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is celebrating her new haircut by listening to...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/07/09 11:41 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>has a crush on the Green Lantern.</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/15/09 07:48 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>has magic on the brain.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/05/09 04:27 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>saw Transformers, Up, and Year One this week. ...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/25/09 04:36 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>Who wants to meet up on schedule pick-up day a...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/11/09 03:38 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>desires the thrill of inspiration. Also, money.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/07/09 09:35 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is going to bed at 9:30! Yeah!</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/18/09 01:29 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is reading, admiring her permit, and occasiona...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/21/09 10:18 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>thinks intangibility should be an option in re...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/31/09 02:27 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is tired. *PROPNAME*, let me go to sleep pl0x.</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/11/09 05:44 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is discovering the many flavors of insomnia.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/27/09 06:59 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is watching cousin play computer game on telev...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/16/09 04:52 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>Why is it I'm only getting the urge to draw wh...</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.786</td>\n",
       "      <td>-0.6841</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/26/09 12:16 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>Who'da thought a single text message could be ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/30/09 03:39 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>wishes to develop a super power that prevents ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>09/01/09 02:05 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>TELL ME WHAT TO DRAW, PLOX.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>08/02/09 09:14 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>found a bunny, bunny died, buried bunny, now i...</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/26/09 07:53 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>&lt;3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>09/08/09 11:00 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is just about insane by now.</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>09/08/09 09:46 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>wants to sleep eight hours tonight. And so she...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>09/17/09 01:32 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>really hates hormones and emotions right now. ...</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.5095</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>09/08/09 03:04 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.60</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9887</th>\n",
       "      <td>9887</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Seriously, if a guy that you're dating ever sa...</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.4993</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/11/10 09:39 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>9888</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Still waiting....so glad Monday's a holiday!</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/15/10 01:53 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9889</th>\n",
       "      <td>9889</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Why did they discontinue naptime after kinderg...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/19/10 07:23 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>9890</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Yay, finally got my computer set up at my hous...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/17/10 09:11 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>9891</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>UGGGGGGHHHHH! My WoW account is totally screwe...</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.3648</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/18/10 06:59 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9892</th>\n",
       "      <td>9892</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Wowsa....greeeeeeeat job news down the grapevi...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/14/10 05:05 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>9893</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>I feel like I'm drowning in paper work, uggh! :X</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/21/10 03:46 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>9894</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Brrrrrrrrrrrrrrrr, cold day! Where are those 4...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/12/10 04:16 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>9895</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Awesome! First sunny day in ages...but now it'...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.8217</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/24/10 03:49 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>9896</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Woot woot! Good week...super productive and bu...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.9036</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/27/10 04:35 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>9897</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>I hope it snows enough to go sledding...who wa...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/29/10 05:27 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>9898</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Crap, I think my eyes hate me. Far away stuff ...</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.6083</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>02/01/10 09:29 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>9899</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>Rain, rain, go away, don't come back 'til I'm ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>02/07/10 03:54 AM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>9900</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>It's not that I can't live without you; it's t...</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/29/10 10:58 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>9901</td>\n",
       "      <td>d2ee2871aa02698e66fdc72b2218cdd8</td>\n",
       "      <td>It's a beautiful day in a neighborhood, a beau...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/25/10 04:15 PM</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11667.80</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12225.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9902</th>\n",
       "      <td>9902</td>\n",
       "      <td>ea890531d3e0547166efde52d843099e</td>\n",
       "      <td>Just wanted to thank everyone for all the supp...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/06/10 06:25 PM</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49975.10</td>\n",
       "      <td>95.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>51220.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9903</th>\n",
       "      <td>9903</td>\n",
       "      <td>ea890531d3e0547166efde52d843099e</td>\n",
       "      <td>using facebook as procrastination for practicu...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/14/10 06:37 AM</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49975.10</td>\n",
       "      <td>95.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>51220.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>9904</td>\n",
       "      <td>ea890531d3e0547166efde52d843099e</td>\n",
       "      <td>First day quitting smoking...wish me luck!!!</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/05/10 10:28 PM</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49975.10</td>\n",
       "      <td>95.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>51220.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>9905</td>\n",
       "      <td>ea890531d3e0547166efde52d843099e</td>\n",
       "      <td>you know when you work your @$$ off for a real...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>02/02/10 06:13 AM</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49975.10</td>\n",
       "      <td>95.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>51220.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>9906</td>\n",
       "      <td>fbe5aa478508d1dc931427ade5d9e1b4</td>\n",
       "      <td>Totalled my car last night. Luckilly no one wa...</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>12/25/09 04:47 PM</td>\n",
       "      <td>231.0</td>\n",
       "      <td>22856.80</td>\n",
       "      <td>86.79</td>\n",
       "      <td>0.06</td>\n",
       "      <td>24946.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>9907</td>\n",
       "      <td>fbe5aa478508d1dc931427ade5d9e1b4</td>\n",
       "      <td>Oh well.  Only two things to do when your team...</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>01/17/10 04:54 AM</td>\n",
       "      <td>231.0</td>\n",
       "      <td>22856.80</td>\n",
       "      <td>86.79</td>\n",
       "      <td>0.06</td>\n",
       "      <td>24946.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>9908</td>\n",
       "      <td>fbe5aa478508d1dc931427ade5d9e1b4</td>\n",
       "      <td>And the cabin fever begins.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>02/06/10 05:10 PM</td>\n",
       "      <td>231.0</td>\n",
       "      <td>22856.80</td>\n",
       "      <td>86.79</td>\n",
       "      <td>0.06</td>\n",
       "      <td>24946.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>9909</td>\n",
       "      <td>35efb99775d5ee7e83cf7912591984d5</td>\n",
       "      <td>Facebook me marea. Me hates it long time T-T</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>2.45</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.85</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>01/18/10 09:15 AM</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1696.85</td>\n",
       "      <td>84.17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>9910</td>\n",
       "      <td>a764ca41dca158d7a191505dcc8ce47f</td>\n",
       "      <td>Red</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.20</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>01/11/10 04:19 AM</td>\n",
       "      <td>451.0</td>\n",
       "      <td>98153.10</td>\n",
       "      <td>97.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>99733.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>9911</td>\n",
       "      <td>deb899e426c1a5c66c24eeb0d7df6257</td>\n",
       "      <td>About mornings and winter,and magic.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.15</td>\n",
       "      <td>4.10</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>01/21/10 10:04 PM</td>\n",
       "      <td>36.0</td>\n",
       "      <td>185.71</td>\n",
       "      <td>31.21</td>\n",
       "      <td>0.40</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>9912</td>\n",
       "      <td>deb899e426c1a5c66c24eeb0d7df6257</td>\n",
       "      <td>little things give you away.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.15</td>\n",
       "      <td>4.10</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>01/28/10 11:48 AM</td>\n",
       "      <td>36.0</td>\n",
       "      <td>185.71</td>\n",
       "      <td>31.21</td>\n",
       "      <td>0.40</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>9913</td>\n",
       "      <td>ea28a927cb6663480ea33ca917c3c8ba</td>\n",
       "      <td>is wishing it was Saturday.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>4.05</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.80</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>02/02/10 08:08 PM</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2935.76</td>\n",
       "      <td>88.40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3120.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>9914</td>\n",
       "      <td>ea28a927cb6663480ea33ca917c3c8ba</td>\n",
       "      <td>is studying hard for the G.R.E.</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>4.05</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.80</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>02/09/10 08:04 PM</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2935.76</td>\n",
       "      <td>88.40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3120.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>9915</td>\n",
       "      <td>5532642937eb3497a43e15dbb23a9d2d</td>\n",
       "      <td>snipers get more head</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.40</td>\n",
       "      <td>4.05</td>\n",
       "      <td>3.30</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>02/10/10 02:01 AM</td>\n",
       "      <td>154.0</td>\n",
       "      <td>11424.50</td>\n",
       "      <td>98.25</td>\n",
       "      <td>0.02</td>\n",
       "      <td>11510.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>9916</td>\n",
       "      <td>a286b7286b1247d4a7851709e9f31e1e</td>\n",
       "      <td>Last night was amazing! Not only did I see *PR...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>02/08/10 06:28 PM</td>\n",
       "      <td>539.0</td>\n",
       "      <td>138337.00</td>\n",
       "      <td>95.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>142460.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9916 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                           #AUTHID  \\\n",
       "0              0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1              1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2              2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3              3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4              4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "5              5  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "6              6  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "7              7  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "8              8  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "9              9  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "10            10  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "11            11  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "12            12  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "13            13  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "14            14  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "15            15  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "16            16  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "17            17  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "18            18  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "19            19  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "20            20  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "21            21  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "22            22  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "23            23  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "24            24  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "25            25  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "26            26  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "27            27  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "28            28  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "29            29  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "...          ...                               ...   \n",
       "9887        9887  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9888        9888  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9889        9889  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9890        9890  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9891        9891  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9892        9892  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9893        9893  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9894        9894  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9895        9895  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9896        9896  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9897        9897  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9898        9898  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9899        9899  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9900        9900  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9901        9901  d2ee2871aa02698e66fdc72b2218cdd8   \n",
       "9902        9902  ea890531d3e0547166efde52d843099e   \n",
       "9903        9903  ea890531d3e0547166efde52d843099e   \n",
       "9904        9904  ea890531d3e0547166efde52d843099e   \n",
       "9905        9905  ea890531d3e0547166efde52d843099e   \n",
       "9906        9906  fbe5aa478508d1dc931427ade5d9e1b4   \n",
       "9907        9907  fbe5aa478508d1dc931427ade5d9e1b4   \n",
       "9908        9908  fbe5aa478508d1dc931427ade5d9e1b4   \n",
       "9909        9909  35efb99775d5ee7e83cf7912591984d5   \n",
       "9910        9910  a764ca41dca158d7a191505dcc8ce47f   \n",
       "9911        9911  deb899e426c1a5c66c24eeb0d7df6257   \n",
       "9912        9912  deb899e426c1a5c66c24eeb0d7df6257   \n",
       "9913        9913  ea28a927cb6663480ea33ca917c3c8ba   \n",
       "9914        9914  ea28a927cb6663480ea33ca917c3c8ba   \n",
       "9915        9915  5532642937eb3497a43e15dbb23a9d2d   \n",
       "9916        9916  a286b7286b1247d4a7851709e9f31e1e   \n",
       "\n",
       "                                                 STATUS  sentiNEG  sentiPOS  \\\n",
       "0                           likes the sound of thunder.     0.000     0.412   \n",
       "1     is so sleepy it's not even funny that's she ca...     0.167     0.000   \n",
       "2     is sore and wants the knot of muscles at the b...     0.195     0.278   \n",
       "3            likes how the day sounds in this new song.     0.000     0.259   \n",
       "4                                           is home. <3     0.000     0.592   \n",
       "5                                 www.thejokerblogs.com     0.000     0.000   \n",
       "6     saw a nun zombie, and liked it. Also, *PROPNAM...     0.000     0.515   \n",
       "7     is in Kentucky. 421 miles into her 1100 mile j...     0.000     0.000   \n",
       "8     was about to finish a digital painting before ...     0.188     0.053   \n",
       "9     is celebrating her new haircut by listening to...     0.000     0.323   \n",
       "10                    has a crush on the Green Lantern.     0.242     0.000   \n",
       "11                              has magic on the brain.     0.000     0.000   \n",
       "12    saw Transformers, Up, and Year One this week. ...     0.136     0.376   \n",
       "13    Who wants to meet up on schedule pick-up day a...     0.000     0.000   \n",
       "14      desires the thrill of inspiration. Also, money.     0.000     0.541   \n",
       "15                       is going to bed at 9:30! Yeah!     0.000     0.317   \n",
       "16    is reading, admiring her permit, and occasiona...     0.000     0.178   \n",
       "17    thinks intangibility should be an option in re...     0.000     0.000   \n",
       "18       is tired. *PROPNAME*, let me go to sleep pl0x.     0.266     0.000   \n",
       "19         is discovering the many flavors of insomnia.     0.000     0.000   \n",
       "20    is watching cousin play computer game on telev...     0.000     0.179   \n",
       "21    Why is it I'm only getting the urge to draw wh...     0.214     0.000   \n",
       "22    Who'da thought a single text message could be ...     0.000     0.000   \n",
       "23    wishes to develop a super power that prevents ...     0.000     0.430   \n",
       "24                          TELL ME WHAT TO DRAW, PLOX.     0.000     0.000   \n",
       "25    found a bunny, bunny died, buried bunny, now i...     0.310     0.000   \n",
       "26                                                   <3     0.000     1.000   \n",
       "27                         is just about insane by now.     0.351     0.000   \n",
       "28    wants to sleep eight hours tonight. And so she...     0.000     0.000   \n",
       "29    really hates hormones and emotions right now. ...     0.329     0.227   \n",
       "...                                                 ...       ...       ...   \n",
       "9887  Seriously, if a guy that you're dating ever sa...     0.144     0.000   \n",
       "9888       Still waiting....so glad Monday's a holiday!     0.000     0.666   \n",
       "9889  Why did they discontinue naptime after kinderg...     0.000     0.000   \n",
       "9890  Yay, finally got my computer set up at my hous...     0.000     0.163   \n",
       "9891  UGGGGGGHHHHH! My WoW account is totally screwe...     0.262     0.158   \n",
       "9892  Wowsa....greeeeeeeat job news down the grapevi...     0.000     0.000   \n",
       "9893   I feel like I'm drowning in paper work, uggh! :X     0.000     0.259   \n",
       "9894  Brrrrrrrrrrrrrrrr, cold day! Where are those 4...     0.000     0.000   \n",
       "9895  Awesome! First sunny day in ages...but now it'...     0.136     0.490   \n",
       "9896  Woot woot! Good week...super productive and bu...     0.000     0.603   \n",
       "9897  I hope it snows enough to go sledding...who wa...     0.000     0.225   \n",
       "9898  Crap, I think my eyes hate me. Far away stuff ...     0.407     0.179   \n",
       "9899  Rain, rain, go away, don't come back 'til I'm ...     0.000     0.000   \n",
       "9900  It's not that I can't live without you; it's t...     0.086     0.000   \n",
       "9901  It's a beautiful day in a neighborhood, a beau...     0.000     0.394   \n",
       "9902  Just wanted to thank everyone for all the supp...     0.000     0.413   \n",
       "9903  using facebook as procrastination for practicu...     0.000     0.286   \n",
       "9904       First day quitting smoking...wish me luck!!!     0.000     0.437   \n",
       "9905  you know when you work your @$$ off for a real...     0.000     0.107   \n",
       "9906  Totalled my car last night. Luckilly no one wa...     0.196     0.324   \n",
       "9907  Oh well.  Only two things to do when your team...     0.108     0.081   \n",
       "9908                        And the cabin fever begins.     0.000     0.000   \n",
       "9909       Facebook me marea. Me hates it long time T-T     0.266     0.000   \n",
       "9910                                                Red     0.000     0.000   \n",
       "9911               About mornings and winter,and magic.     0.000     0.000   \n",
       "9912                       little things give you away.     0.000     0.000   \n",
       "9913                        is wishing it was Saturday.     0.000     0.322   \n",
       "9914                    is studying hard for the G.R.E.     0.219     0.000   \n",
       "9915                              snipers get more head     0.000     0.000   \n",
       "9916  Last night was amazing! Not only did I see *PR...     0.000     0.223   \n",
       "\n",
       "      sentiNEU  sentiCOMPOUND  sEXT  sNEU  sAGR      ...       cCON  cOPN  \\\n",
       "0        0.588         0.4215  2.65  3.00  3.15      ...          n     y   \n",
       "1        0.833        -0.3412  2.65  3.00  3.15      ...          n     y   \n",
       "2        0.527         0.6280  2.65  3.00  3.15      ...          n     y   \n",
       "3        0.741         0.4215  2.65  3.00  3.15      ...          n     y   \n",
       "4        0.408         0.4404  2.65  3.00  3.15      ...          n     y   \n",
       "5        1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "6        0.485         0.8916  2.65  3.00  3.15      ...          n     y   \n",
       "7        1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "8        0.759        -0.6249  2.65  3.00  3.15      ...          n     y   \n",
       "9        0.677         0.7351  2.65  3.00  3.15      ...          n     y   \n",
       "10       0.758        -0.1531  2.65  3.00  3.15      ...          n     y   \n",
       "11       1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "12       0.488         0.6633  2.65  3.00  3.15      ...          n     y   \n",
       "13       1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "14       0.459         0.7096  2.65  3.00  3.15      ...          n     y   \n",
       "15       0.683         0.4184  2.65  3.00  3.15      ...          n     y   \n",
       "16       0.822         0.3818  2.65  3.00  3.15      ...          n     y   \n",
       "17       1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "18       0.734        -0.4404  2.65  3.00  3.15      ...          n     y   \n",
       "19       1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "20       0.821         0.3400  2.65  3.00  3.15      ...          n     y   \n",
       "21       0.786        -0.6841  2.65  3.00  3.15      ...          n     y   \n",
       "22       1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "23       0.570         0.7003  2.65  3.00  3.15      ...          n     y   \n",
       "24       1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "25       0.690        -0.5574  2.65  3.00  3.15      ...          n     y   \n",
       "26       0.000         0.4404  2.65  3.00  3.15      ...          n     y   \n",
       "27       0.649        -0.4019  2.65  3.00  3.15      ...          n     y   \n",
       "28       1.000         0.0000  2.65  3.00  3.15      ...          n     y   \n",
       "29       0.445        -0.5095  2.65  3.00  3.15      ...          n     y   \n",
       "...        ...            ...   ...   ...   ...      ...        ...   ...   \n",
       "9887     0.856        -0.4993  2.75  3.25  2.50      ...          y     y   \n",
       "9888     0.334         0.7177  2.75  3.25  2.50      ...          y     y   \n",
       "9889     1.000         0.0000  2.75  3.25  2.50      ...          y     y   \n",
       "9890     0.837         0.5707  2.75  3.25  2.50      ...          y     y   \n",
       "9891     0.580        -0.3648  2.75  3.25  2.50      ...          y     y   \n",
       "9892     1.000         0.0000  2.75  3.25  2.50      ...          y     y   \n",
       "9893     0.741         0.4199  2.75  3.25  2.50      ...          y     y   \n",
       "9894     1.000         0.0000  2.75  3.25  2.50      ...          y     y   \n",
       "9895     0.374         0.8217  2.75  3.25  2.50      ...          y     y   \n",
       "9896     0.397         0.9036  2.75  3.25  2.50      ...          y     y   \n",
       "9897     0.775         0.5399  2.75  3.25  2.50      ...          y     y   \n",
       "9898     0.413        -0.6083  2.75  3.25  2.50      ...          y     y   \n",
       "9899     1.000         0.0000  2.75  3.25  2.50      ...          y     y   \n",
       "9900     0.914        -0.0572  2.75  3.25  2.50      ...          y     y   \n",
       "9901     0.606         0.8316  2.75  3.25  2.50      ...          y     y   \n",
       "9902     0.587         0.9449  4.75  4.25  3.75      ...          y     y   \n",
       "9903     0.714         0.4215  4.75  4.25  3.75      ...          y     y   \n",
       "9904     0.563         0.5962  4.75  4.25  3.75      ...          y     y   \n",
       "9905     0.893         0.4588  4.75  4.25  3.75      ...          y     y   \n",
       "9906     0.480         0.4939  3.25  2.75  2.25      ...          y     y   \n",
       "9907     0.811        -0.1779  3.25  2.75  2.25      ...          y     y   \n",
       "9908     1.000         0.0000  3.25  2.75  2.25      ...          y     y   \n",
       "9909     0.734        -0.4404  2.45  4.00  2.85      ...          n     y   \n",
       "9910     1.000         0.0000  3.70  2.50  4.20      ...          y     n   \n",
       "9911     1.000         0.0000  2.15  2.15  4.10      ...          n     y   \n",
       "9912     1.000         0.0000  2.15  2.15  4.10      ...          n     y   \n",
       "9913     0.678         0.2263  4.05  3.35  3.80      ...          y     y   \n",
       "9914     0.781        -0.1027  4.05  3.35  3.80      ...          y     y   \n",
       "9915     1.000         0.0000  1.40  4.05  3.30      ...          n     y   \n",
       "9916     0.777         0.8416  4.25  3.00  3.25      ...          y     y   \n",
       "\n",
       "                   DATE NETWORKSIZE BETWEENNESS NBETWEENNESS DENSITY  \\\n",
       "0     06/19/09 03:21 PM       180.0    14861.60        93.29    0.03   \n",
       "1     07/02/09 08:41 AM       180.0    14861.60        93.29    0.03   \n",
       "2     06/15/09 01:15 PM       180.0    14861.60        93.29    0.03   \n",
       "3     06/22/09 04:48 AM       180.0    14861.60        93.29    0.03   \n",
       "4     07/20/09 02:31 AM       180.0    14861.60        93.29    0.03   \n",
       "5     07/16/09 03:21 PM       180.0    14861.60        93.29    0.03   \n",
       "6     06/27/09 05:41 AM       180.0    14861.60        93.29    0.03   \n",
       "7     07/18/09 06:34 AM       180.0    14861.60        93.29    0.03   \n",
       "8     07/09/09 02:58 PM       180.0    14861.60        93.29    0.03   \n",
       "9     07/07/09 11:41 PM       180.0    14861.60        93.29    0.03   \n",
       "10    07/15/09 07:48 PM       180.0    14861.60        93.29    0.03   \n",
       "11    08/05/09 04:27 AM       180.0    14861.60        93.29    0.03   \n",
       "12    06/25/09 04:36 AM       180.0    14861.60        93.29    0.03   \n",
       "13    08/11/09 03:38 AM       180.0    14861.60        93.29    0.03   \n",
       "14    08/07/09 09:35 PM       180.0    14861.60        93.29    0.03   \n",
       "15    08/18/09 01:29 AM       180.0    14861.60        93.29    0.03   \n",
       "16    08/21/09 10:18 PM       180.0    14861.60        93.29    0.03   \n",
       "17    08/31/09 02:27 AM       180.0    14861.60        93.29    0.03   \n",
       "18    07/11/09 05:44 AM       180.0    14861.60        93.29    0.03   \n",
       "19    07/27/09 06:59 AM       180.0    14861.60        93.29    0.03   \n",
       "20    06/16/09 04:52 AM       180.0    14861.60        93.29    0.03   \n",
       "21    08/26/09 12:16 AM       180.0    14861.60        93.29    0.03   \n",
       "22    08/30/09 03:39 AM       180.0    14861.60        93.29    0.03   \n",
       "23    09/01/09 02:05 AM       180.0    14861.60        93.29    0.03   \n",
       "24    08/02/09 09:14 AM       180.0    14861.60        93.29    0.03   \n",
       "25    07/26/09 07:53 PM       180.0    14861.60        93.29    0.03   \n",
       "26    09/08/09 11:00 PM       180.0    14861.60        93.29    0.03   \n",
       "27    09/08/09 09:46 PM       180.0    14861.60        93.29    0.03   \n",
       "28    09/17/09 01:32 AM       180.0    14861.60        93.29    0.03   \n",
       "29    09/08/09 03:04 AM       180.0    14861.60        93.29    0.03   \n",
       "...                 ...         ...         ...          ...     ...   \n",
       "9887  01/11/10 09:39 PM       161.0    11667.80        91.73    0.05   \n",
       "9888  01/15/10 01:53 PM       161.0    11667.80        91.73    0.05   \n",
       "9889  01/19/10 07:23 PM       161.0    11667.80        91.73    0.05   \n",
       "9890  01/17/10 09:11 PM       161.0    11667.80        91.73    0.05   \n",
       "9891  01/18/10 06:59 PM       161.0    11667.80        91.73    0.05   \n",
       "9892  01/14/10 05:05 PM       161.0    11667.80        91.73    0.05   \n",
       "9893  01/21/10 03:46 PM       161.0    11667.80        91.73    0.05   \n",
       "9894  01/12/10 04:16 PM       161.0    11667.80        91.73    0.05   \n",
       "9895  01/24/10 03:49 PM       161.0    11667.80        91.73    0.05   \n",
       "9896  01/27/10 04:35 PM       161.0    11667.80        91.73    0.05   \n",
       "9897  01/29/10 05:27 PM       161.0    11667.80        91.73    0.05   \n",
       "9898  02/01/10 09:29 PM       161.0    11667.80        91.73    0.05   \n",
       "9899  02/07/10 03:54 AM       161.0    11667.80        91.73    0.05   \n",
       "9900  01/29/10 10:58 PM       161.0    11667.80        91.73    0.05   \n",
       "9901  01/25/10 04:15 PM       161.0    11667.80        91.73    0.05   \n",
       "9902  01/06/10 06:25 PM       325.0    49975.10        95.51    0.03   \n",
       "9903  01/14/10 06:37 AM       325.0    49975.10        95.51    0.03   \n",
       "9904  01/05/10 10:28 PM       325.0    49975.10        95.51    0.03   \n",
       "9905  02/02/10 06:13 AM       325.0    49975.10        95.51    0.03   \n",
       "9906  12/25/09 04:47 PM       231.0    22856.80        86.79    0.06   \n",
       "9907  01/17/10 04:54 AM       231.0    22856.80        86.79    0.06   \n",
       "9908  02/06/10 05:10 PM       231.0    22856.80        86.79    0.06   \n",
       "9909  01/18/10 09:15 AM        65.0     1696.85        84.17    0.10   \n",
       "9910  01/11/10 04:19 AM       451.0    98153.10        97.16    0.02   \n",
       "9911  01/21/10 10:04 PM        36.0      185.71        31.21    0.40   \n",
       "9912  01/28/10 11:48 AM        36.0      185.71        31.21    0.40   \n",
       "9913  02/02/10 08:08 PM        83.0     2935.76        88.40    0.08   \n",
       "9914  02/09/10 08:04 PM        83.0     2935.76        88.40    0.08   \n",
       "9915  02/10/10 02:01 AM       154.0    11424.50        98.25    0.02   \n",
       "9916  02/08/10 06:28 PM       539.0   138337.00        95.77    0.02   \n",
       "\n",
       "     BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0      15661.0        0.49          0.10  \n",
       "1      15661.0        0.49          0.10  \n",
       "2      15661.0        0.49          0.10  \n",
       "3      15661.0        0.49          0.10  \n",
       "4      15661.0        0.49          0.10  \n",
       "5      15661.0        0.49          0.10  \n",
       "6      15661.0        0.49          0.10  \n",
       "7      15661.0        0.49          0.10  \n",
       "8      15661.0        0.49          0.10  \n",
       "9      15661.0        0.49          0.10  \n",
       "10     15661.0        0.49          0.10  \n",
       "11     15661.0        0.49          0.10  \n",
       "12     15661.0        0.49          0.10  \n",
       "13     15661.0        0.49          0.10  \n",
       "14     15661.0        0.49          0.10  \n",
       "15     15661.0        0.49          0.10  \n",
       "16     15661.0        0.49          0.10  \n",
       "17     15661.0        0.49          0.10  \n",
       "18     15661.0        0.49          0.10  \n",
       "19     15661.0        0.49          0.10  \n",
       "20     15661.0        0.49          0.10  \n",
       "21     15661.0        0.49          0.10  \n",
       "22     15661.0        0.49          0.10  \n",
       "23     15661.0        0.49          0.10  \n",
       "24     15661.0        0.49          0.10  \n",
       "25     15661.0        0.49          0.10  \n",
       "26     15661.0        0.49          0.10  \n",
       "27     15661.0        0.49          0.10  \n",
       "28     15661.0        0.49          0.10  \n",
       "29     15661.0        0.49          0.10  \n",
       "...        ...         ...           ...  \n",
       "9887   12225.0        0.48          0.26  \n",
       "9888   12225.0        0.48          0.26  \n",
       "9889   12225.0        0.48          0.26  \n",
       "9890   12225.0        0.48          0.26  \n",
       "9891   12225.0        0.48          0.26  \n",
       "9892   12225.0        0.48          0.26  \n",
       "9893   12225.0        0.48          0.26  \n",
       "9894   12225.0        0.48          0.26  \n",
       "9895   12225.0        0.48          0.26  \n",
       "9896   12225.0        0.48          0.26  \n",
       "9897   12225.0        0.48          0.26  \n",
       "9898   12225.0        0.48          0.26  \n",
       "9899   12225.0        0.48          0.26  \n",
       "9900   12225.0        0.48          0.26  \n",
       "9901   12225.0        0.48          0.26  \n",
       "9902   51220.0        0.49          0.18  \n",
       "9903   51220.0        0.49          0.18  \n",
       "9904   51220.0        0.49          0.18  \n",
       "9905   51220.0        0.49          0.18  \n",
       "9906   24946.0        0.47          0.36  \n",
       "9907   24946.0        0.47          0.36  \n",
       "9908   24946.0        0.47          0.36  \n",
       "9909    1876.0        0.47          0.28  \n",
       "9910   99733.0        0.49          0.12  \n",
       "9911     377.0        0.32          0.63  \n",
       "9912     377.0        0.32          0.63  \n",
       "9913    3120.0        0.47          0.26  \n",
       "9914    3120.0        0.47          0.26  \n",
       "9915   11510.0        0.49          0.05  \n",
       "9916  142460.0        0.49          0.13  \n",
       "\n",
       "[9916 rows x 25 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(status_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             likes the sound of thunder.\n",
       "1       is so sleepy it's not even funny that's she ca...\n",
       "2       is sore and wants the knot of muscles at the b...\n",
       "3              likes how the day sounds in this new song.\n",
       "4                                             is home. <3\n",
       "5                                   www.thejokerblogs.com\n",
       "6       saw a nun zombie, and liked it. Also, *PROPNAM...\n",
       "7       is in Kentucky. 421 miles into her 1100 mile j...\n",
       "8       was about to finish a digital painting before ...\n",
       "9       is celebrating her new haircut by listening to...\n",
       "10                      has a crush on the Green Lantern.\n",
       "11                                has magic on the brain.\n",
       "12      saw Transformers, Up, and Year One this week. ...\n",
       "13      Who wants to meet up on schedule pick-up day a...\n",
       "14        desires the thrill of inspiration. Also, money.\n",
       "15                         is going to bed at 9:30! Yeah!\n",
       "16      is reading, admiring her permit, and occasiona...\n",
       "17      thinks intangibility should be an option in re...\n",
       "18         is tired. *PROPNAME*, let me go to sleep pl0x.\n",
       "19           is discovering the many flavors of insomnia.\n",
       "20      is watching cousin play computer game on telev...\n",
       "21      Why is it I'm only getting the urge to draw wh...\n",
       "22      Who'da thought a single text message could be ...\n",
       "23      wishes to develop a super power that prevents ...\n",
       "24                            TELL ME WHAT TO DRAW, PLOX.\n",
       "25      found a bunny, bunny died, buried bunny, now i...\n",
       "26                                                     <3\n",
       "27                           is just about insane by now.\n",
       "28      wants to sleep eight hours tonight. And so she...\n",
       "29      really hates hormones and emotions right now. ...\n",
       "                              ...                        \n",
       "3549    \"Beyond mountains there are mountains.\" - Hait...\n",
       "3550    ?Hell is empty and all the devils are here.? S...\n",
       "3551                                     cheer up emo kid\n",
       "3552                                          So it goes.\n",
       "3553    When the student is ready, the teacher will ap...\n",
       "3554    nothing like having stuff stolen out of your c...\n",
       "3555    apparently my fb account emailed a bunch of pe...\n",
       "3556           and now for somethign completely different\n",
       "3557                                    back in the habit\n",
       "3558                 Friday why do you come on so strong?\n",
       "3559    well survived the city and SFO in the pouring ...\n",
       "3560    Good to see Tahoe kids pulling some medals. Co...\n",
       "3561    We're the tattooed lady, and we're never going...\n",
       "3562           is excited to pop chuckies natives cherry!\n",
       "3563    If you read this, even if we don't speak often...\n",
       "3564               has an interview in 30...SO NERVOUS!!!\n",
       "3565                          out for the day with my man\n",
       "3566    is definitely a nerd.  just freaked out on jay...\n",
       "3567       was looking for a job and ended up with two...\n",
       "3568    Dear rain, I have a wonderful day of boating a...\n",
       "3569    anyone wanna join me, renee and chaz at the ba...\n",
       "3570    going out with the other interns and renee?  t...\n",
       "3571    things to remember while watching a scary movi...\n",
       "3572    a reggae band covering one of the best beatles...\n",
       "3573    we all threw it back forcefully to THE NATIVES...\n",
       "3574    what an adventurous night...filled with the na...\n",
       "3575    another adventurous night with the natives...g...\n",
       "3576                         good bye el...until sunday!!\n",
       "3577    i want to go to the bar tonight...is that too ...\n",
       "3578    i woke up with the roots stuck in my head...weird\n",
       "Name: STATUS, Length: 3579, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_data.ix[0:3578,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into training and test data: 66% and 33%\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_data, test_data = train_test_split(status_data, test_size=0.50)\n",
    "\n",
    "train_data = status_data.ix[0:3577,2]\n",
    "test_data = status_data.ix[3577:,2]\n",
    "\n",
    "train = train_data.values\n",
    "test = test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             likes the sound of thunder.\n",
       "1       is so sleepy it's not even funny that's she ca...\n",
       "2       is sore and wants the knot of muscles at the b...\n",
       "3              likes how the day sounds in this new song.\n",
       "4                                             is home. <3\n",
       "5                                   www.thejokerblogs.com\n",
       "6       saw a nun zombie, and liked it. Also, *PROPNAM...\n",
       "7       is in Kentucky. 421 miles into her 1100 mile j...\n",
       "8       was about to finish a digital painting before ...\n",
       "9       is celebrating her new haircut by listening to...\n",
       "10                      has a crush on the Green Lantern.\n",
       "11                                has magic on the brain.\n",
       "12      saw Transformers, Up, and Year One this week. ...\n",
       "13      Who wants to meet up on schedule pick-up day a...\n",
       "14        desires the thrill of inspiration. Also, money.\n",
       "15                         is going to bed at 9:30! Yeah!\n",
       "16      is reading, admiring her permit, and occasiona...\n",
       "17      thinks intangibility should be an option in re...\n",
       "18         is tired. *PROPNAME*, let me go to sleep pl0x.\n",
       "19           is discovering the many flavors of insomnia.\n",
       "20      is watching cousin play computer game on telev...\n",
       "21      Why is it I'm only getting the urge to draw wh...\n",
       "22      Who'da thought a single text message could be ...\n",
       "23      wishes to develop a super power that prevents ...\n",
       "24                            TELL ME WHAT TO DRAW, PLOX.\n",
       "25      found a bunny, bunny died, buried bunny, now i...\n",
       "26                                                     <3\n",
       "27                           is just about insane by now.\n",
       "28      wants to sleep eight hours tonight. And so she...\n",
       "29      really hates hormones and emotions right now. ...\n",
       "                              ...                        \n",
       "3548                         monday morning= broken brain\n",
       "3549    \"Beyond mountains there are mountains.\" - Hait...\n",
       "3550    ?Hell is empty and all the devils are here.? S...\n",
       "3551                                     cheer up emo kid\n",
       "3552                                          So it goes.\n",
       "3553    When the student is ready, the teacher will ap...\n",
       "3554    nothing like having stuff stolen out of your c...\n",
       "3555    apparently my fb account emailed a bunch of pe...\n",
       "3556           and now for somethign completely different\n",
       "3557                                    back in the habit\n",
       "3558                 Friday why do you come on so strong?\n",
       "3559    well survived the city and SFO in the pouring ...\n",
       "3560    Good to see Tahoe kids pulling some medals. Co...\n",
       "3561    We're the tattooed lady, and we're never going...\n",
       "3562           is excited to pop chuckies natives cherry!\n",
       "3563    If you read this, even if we don't speak often...\n",
       "3564               has an interview in 30...SO NERVOUS!!!\n",
       "3565                          out for the day with my man\n",
       "3566    is definitely a nerd.  just freaked out on jay...\n",
       "3567       was looking for a job and ended up with two...\n",
       "3568    Dear rain, I have a wonderful day of boating a...\n",
       "3569    anyone wanna join me, renee and chaz at the ba...\n",
       "3570    going out with the other interns and renee?  t...\n",
       "3571    things to remember while watching a scary movi...\n",
       "3572    a reggae band covering one of the best beatles...\n",
       "3573    we all threw it back forcefully to THE NATIVES...\n",
       "3574    what an adventurous night...filled with the na...\n",
       "3575    another adventurous night with the natives...g...\n",
       "3576                         good bye el...until sunday!!\n",
       "3577    i want to go to the bar tonight...is that too ...\n",
       "Name: STATUS, Length: 3578, dtype: object"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3577    i want to go to the bar tonight...is that too ...\n",
       "3578    i woke up with the roots stuck in my head...weird\n",
       "3579    i feel like my insides have turned into goo an...\n",
       "3580    is starting to like her new room in the baseme...\n",
       "3581    having people over tonight...call me if you wa...\n",
       "3582    GOING ON AN ADVENTURE...to kalamazoo!!!  bitch...\n",
       "3583              ugh...thursday can not come fast enough\n",
       "3584                                 ugh...i need a job:(\n",
       "3585    finally...the inspection is over and the house...\n",
       "3586    is pumped to see harry potter with chaz...you ...\n",
       "3587                  had a wonderful two days on boat...\n",
       "3588    what a freaking wonderful night...in both good...\n",
       "3589                                 is getting a cold...\n",
       "3590    how did i go from having nothing to do all the...\n",
       "3591         party at my place.  call the cell to join...\n",
       "3592                         karma is a bitch isn't it???\n",
       "3593                        goodbye EL...see you tomorrow\n",
       "3594                    home alone...and terribly bored:(\n",
       "3595                      is completely uncomfortable....\n",
       "3596    going to see inglourious basterds with the boy...\n",
       "3597    partying at my place tonight...call me if you ...\n",
       "3598                                        NACHOFEST!!!!\n",
       "3599    i wish things could change...sadly i know they...\n",
       "3600    not wearing a bra to the bar is a TERRIBLE ide...\n",
       "3601       will be at the library for the rest of life...\n",
       "3602    just realized that i will never get a break fr...\n",
       "3603    so my phone decide that it isn't accepting pho...\n",
       "3604    up earlier than most everyone in EL and trying...\n",
       "3605                  awesome way to start the week...NOT\n",
       "3606    is having a competition with the man to see wh...\n",
       "                              ...                        \n",
       "9887    Seriously, if a guy that you're dating ever sa...\n",
       "9888         Still waiting....so glad Monday's a holiday!\n",
       "9889    Why did they discontinue naptime after kinderg...\n",
       "9890    Yay, finally got my computer set up at my hous...\n",
       "9891    UGGGGGGHHHHH! My WoW account is totally screwe...\n",
       "9892    Wowsa....greeeeeeeat job news down the grapevi...\n",
       "9893     I feel like I'm drowning in paper work, uggh! :X\n",
       "9894    Brrrrrrrrrrrrrrrr, cold day! Where are those 4...\n",
       "9895    Awesome! First sunny day in ages...but now it'...\n",
       "9896    Woot woot! Good week...super productive and bu...\n",
       "9897    I hope it snows enough to go sledding...who wa...\n",
       "9898    Crap, I think my eyes hate me. Far away stuff ...\n",
       "9899    Rain, rain, go away, don't come back 'til I'm ...\n",
       "9900    It's not that I can't live without you; it's t...\n",
       "9901    It's a beautiful day in a neighborhood, a beau...\n",
       "9902    Just wanted to thank everyone for all the supp...\n",
       "9903    using facebook as procrastination for practicu...\n",
       "9904         First day quitting smoking...wish me luck!!!\n",
       "9905    you know when you work your @$$ off for a real...\n",
       "9906    Totalled my car last night. Luckilly no one wa...\n",
       "9907    Oh well.  Only two things to do when your team...\n",
       "9908                          And the cabin fever begins.\n",
       "9909         Facebook me marea. Me hates it long time T-T\n",
       "9910                                                  Red\n",
       "9911                 About mornings and winter,and magic.\n",
       "9912                         little things give you away.\n",
       "9913                          is wishing it was Saturday.\n",
       "9914                      is studying hard for the G.R.E.\n",
       "9915                                snipers get more head\n",
       "9916    Last night was amazing! Not only did I see *PR...\n",
       "Name: STATUS, Length: 6339, dtype: object"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\preprocessing\\text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "sequences_train = tokenizer.texts_to_sequences(train_data)\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (3578, 80)\n",
      "X_test shape: (6339, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "from keras.preprocessing import sequence\n",
    "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "Y_train = np_utils.to_categorical(train_labels, nb_classes)\n",
    "Y_test = np_utils.to_categorical(test_labels, nb_classes)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  app.launch_new_instance()\n",
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, recurrent_dropout=0.2, dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 6339 input samples and 1790 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-f28b4ccb692e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n\u001b[1;32m---> 14\u001b[1;33m           validation_data=(X_test, Y_test))\n\u001b[0m\u001b[0;32m     15\u001b[0m score, acc = model.evaluate(X_test, Y_test,\n\u001b[0;32m     16\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1541\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1543\u001b[1;33m                 batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1544\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m                 \u001b[0mval_ins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1388\u001b[0m                           \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[1;32m-> 1390\u001b[1;33m         \u001b[0m_check_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[0;32m   1392\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    239\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 6339 input samples and 1790 target samples."
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
